{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 图像分类\n",
    "\n",
    "在此项目中，你将对 [CIFAR-10 数据集](https://www.cs.toronto.edu/~kriz/cifar.html) 中的图片进行分类。该数据集包含飞机、猫狗和其他物体。你需要预处理这些图片，然后用所有样本训练一个卷积神经网络。图片需要标准化（normalized），标签需要采用 one-hot 编码。你需要应用所学的知识构建卷积的、最大池化（max pooling）、丢弃（dropout）和完全连接（fully connected）的层。最后，你需要在样本图片上看到神经网络的预测结果。\n",
    "\n",
    "\n",
    "## 获取数据\n",
    "\n",
    "请运行以下单元，以下载 [CIFAR-10 数据集（Python版）](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索数据\n",
    "\n",
    "该数据集分成了几部分／批次（batches），以免你的机器在计算时内存不足。CIFAR-10 数据集包含 5 个部分，名称分别为 `data_batch_1`、`data_batch_2`，以此类推。每个部分都包含以下某个类别的标签和图片：\n",
    "\n",
    "* 飞机\n",
    "* 汽车\n",
    "* 鸟类\n",
    "* 猫\n",
    "* 鹿\n",
    "* 狗\n",
    "* 青蛙\n",
    "* 马\n",
    "* 船只\n",
    "* 卡车\n",
    "\n",
    "了解数据集也是对数据进行预测的必经步骤。你可以通过更改 `batch_id` 和 `sample_id` 探索下面的代码单元。`batch_id` 是数据集一个部分的 ID（1 到 5）。`sample_id` 是该部分中图片和标签对（label pair）的 ID。\n",
    "\n",
    "问问你自己：“可能的标签有哪些？”、“图片数据的值范围是多少？”、“标签是按顺序排列，还是随机排列的？”。思考类似的问题，有助于你预处理数据，并使预测结果更准确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff314f892e8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现预处理函数\n",
    "\n",
    "### 标准化\n",
    "\n",
    "在下面的单元中，实现 `normalize` 函数，传入图片数据 `x`，并返回标准化 Numpy 数组。值应该在 0 到 1 的范围内（含 0 和 1）。返回对象应该和 `x` 的形状一样。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "#     shape = x.shape\n",
    "#     x_flatten = x.astype(np.float32).flatten()\n",
    "#     min_value, max_value = min(x_flatten), max(x_flatten)\n",
    "#     for i, item in enumerate(x_flatten):\n",
    "#         x_flatten[i] = (item - min_value) / (max_value - min_value)\n",
    "#     return x_flatten.reshape(shape)\n",
    "\n",
    "\n",
    "#     x_ = x.astype(np.float32).reshape(-1, x.shape[3])\n",
    "#     out = sklearn.preprocessing.normalize(x_)\n",
    "#     return out.reshape(x.shape)\n",
    "    \n",
    "    return x.astype(np.float32)/255\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot 编码\n",
    "\n",
    "和之前的代码单元一样，你将为预处理实现一个函数。这次，你将实现 `one_hot_encode` 函数。输入，也就是 `x`，是一个标签列表。实现该函数，以返回为 one_hot 编码的 Numpy 数组的标签列表。标签的可能值为 0 到 9。每次调用 `one_hot_encode` 时，对于每个值，one_hot 编码函数应该返回相同的编码。确保将编码映射保存到该函数外面。\n",
    "\n",
    "提示：不要重复发明轮子。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "one_hot_encoder = LabelBinarizer()\n",
    "one_hot_encoder.fit(range(0,10))\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement \n",
    "    \n",
    "    return one_hot_encoder.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机化数据\n",
    "\n",
    "之前探索数据时，你已经了解到，样本的顺序是随机的。再随机化一次也不会有什么关系，但是对于这个数据集没有必要。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理所有数据并保存\n",
    "\n",
    "运行下方的代码单元，将预处理所有 CIFAR-10 数据，并保存到文件中。下面的代码还使用了 10% 的训练数据，用来验证。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点\n",
    "\n",
    "这是你的第一个检查点。如果你什么时候决定再回到该记事本，或需要重新启动该记事本，你可以从这里开始。预处理的数据已保存到本地。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 32, 32, 3), (5000, 10))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_features.shape, valid_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建网络\n",
    "\n",
    "对于该神经网络，你需要将每层都构建为一个函数。你看到的大部分代码都位于函数外面。要更全面地测试你的代码，我们需要你将每层放入一个函数中。这样使我们能够提供更好的反馈，并使用我们的统一测试检测简单的错误，然后再提交项目。\n",
    "\n",
    ">**注意**：如果你觉得每周很难抽出足够的时间学习这门课程，我们为此项目提供了一个小捷径。对于接下来的几个问题，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 程序包中的类来构建每个层级，但是“卷积和最大池化层级”部分的层级除外。TF Layers 和 Keras 及 TFLearn 层级类似，因此很容易学会。\n",
    "\n",
    ">但是，如果你想充分利用这门课程，请尝试自己解决所有问题，不使用 TF Layers 程序包中的任何类。你依然可以使用其他程序包中的类，这些类和你在 TF Layers 中的类名称是一样的！例如，你可以使用 TF Neural Network 版本的 `conv2d` 类 [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)，而不是 TF Layers 版本的 `conv2d` 类 [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d)。\n",
    "\n",
    "我们开始吧！\n",
    "\n",
    "\n",
    "### 输入\n",
    "\n",
    "神经网络需要读取图片数据、one-hot 编码标签和丢弃保留概率（dropout keep probability）。请实现以下函数：\n",
    "\n",
    "* 实现 `neural_net_image_input`\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * 使用 `image_shape` 设置形状，部分大小设为 `None`\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 TensorFlow `name` 参数对 TensorFlow 占位符 \"x\" 命名\n",
    "* 实现 `neural_net_label_input`\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * 使用 `n_classes` 设置形状，部分大小设为 `None`\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 TensorFlow `name` 参数对 TensorFlow 占位符 \"y\" 命名\n",
    "* 实现 `neural_net_keep_prob_input`\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)，用于丢弃保留概率\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 TensorFlow `name` 参数对 TensorFlow 占位符 \"keep_prob\" 命名\n",
    "\n",
    "这些名称将在项目结束时，用于加载保存的模型。\n",
    "\n",
    "注意：TensorFlow 中的 `None` 表示形状可以是动态大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None]+ list(image_shape), name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积和最大池化层\n",
    "\n",
    "卷积层级适合处理图片。对于此代码单元，你应该实现函数 `conv2d_maxpool` 以便应用卷积然后进行最大池化：\n",
    "\n",
    "* 使用 `conv_ksize`、`conv_num_outputs` 和 `x_tensor` 的形状创建权重（weight）和偏置（bias）。\n",
    "* 使用权重和 `conv_strides` 对 `x_tensor` 应用卷积。\n",
    " * 建议使用我们建议的间距（padding），当然也可以使用任何其他间距。\n",
    "* 添加偏置\n",
    "* 向卷积中添加非线性激活（nonlinear activation）\n",
    "* 使用 `pool_ksize` 和 `pool_strides` 应用最大池化\n",
    " * 建议使用我们建议的间距（padding），当然也可以使用任何其他间距。\n",
    "\n",
    "**注意**：对于**此层**，**请勿使用** [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers)，但是仍然可以使用 TensorFlow 的 [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) 包。对于所有**其他层**，你依然可以使用快捷方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides, \n",
    "                   scope=\"conv2d_maxpool\", bn_is_training=tf.constant(True)):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    with tf.variable_scope(scope):\n",
    "        filter_size = [*conv_ksize, int(x_tensor.shape[-1:][0]), conv_num_outputs]\n",
    "#         initializer = tf.random_normal_initializer(stddev=0.05)\n",
    "#         initializer = tf.contrib.layers.xavier_initializer()\n",
    "#         initializer = tf.glorot_uniform_initializer()\n",
    "#         initializer = tf.glorot_normal_initializer()\n",
    "        initializer = tf.contrib.keras.initializers.he_normal()\n",
    "        conv_filter = tf.get_variable(name=\"w\", shape=filter_size, initializer=initializer, dtype=tf.float32)\n",
    "        conv_layer = tf.nn.conv2d(input=x_tensor, \n",
    "                                  filter=conv_filter, \n",
    "                                  strides=[1, *conv_strides, 1], \n",
    "                                  padding=\"SAME\")\n",
    "        bias = tf.get_variable(name=\"b\", shape=[conv_num_outputs], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "        bias_conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "        # batch normal\n",
    "        bn_layer = tf.contrib.layers.batch_norm(bias_conv_layer, center=True, scale=False, is_training=bn_is_training)\n",
    "        \n",
    "        nonliner_conv_layer = tf.nn.relu(bn_layer)\n",
    "\n",
    "        # 使用pool_ksize=(0,0)控制不执行池化\n",
    "        if pool_ksize[0] != 0:\n",
    "            max_pool_layer = tf.nn.max_pool(nonliner_conv_layer, \n",
    "                                        ksize=[1, *pool_ksize, 1], \n",
    "                                        strides=[1, *pool_strides, 1],\n",
    "                                        padding=\"SAME\")\n",
    "            return max_pool_layer \n",
    "        return nonliner_conv_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扁平化层\n",
    "\n",
    "实现 `flatten` 函数，将 `x_tensor` 的维度从四维张量（4-D tensor）变成二维张量。输出应该是形状（*部分大小（Batch Size）*，*扁平化图片大小（Flattened Image Size）*）。快捷方法：对于此层，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的类。如果你想要更大挑战，可以仅使用其他 TensorFlow 程序包。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "#     print(x_tensor.shape)\n",
    "#     print(tf.contrib.layers.flatten(x_tensor).shape)\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全连接层\n",
    "\n",
    "实现 `fully_conn` 函数，以向 `x_tensor` 应用完全连接的层级，形状为（*部分大小（Batch Size）*，*num_outputs*）。快捷方法：对于此层，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的类。如果你想要更大挑战，可以仅使用其他 TensorFlow 程序包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs, scope=\"fc\"):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "#     print(x_tensor.shape)\n",
    "    return tf.contrib.layers.fully_connected(inputs=x_tensor, num_outputs=num_outputs, scope=scope)\n",
    "        #weights_initializer=tf.random_normal_initializer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出层\n",
    "\n",
    "实现 `output` 函数，向 x_tensor 应用完全连接的层级，形状为（*部分大小（Batch Size）*，*num_outputs*）。快捷方法：对于此层，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的类。如果你想要更大挑战，可以仅使用其他 TensorFlow 程序包。\n",
    "\n",
    "**注意**：该层级不应应用 Activation、softmax 或交叉熵（cross entropy）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs, scope=\"output\"):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "#     print(x_tensor.shape)\n",
    "    \n",
    "    return tf.contrib.layers.fully_connected(inputs=x_tensor, num_outputs=num_outputs, activation_fn=None, scope=scope)\n",
    "        # weights_initializer=tf.random_normal_initializer                                            \n",
    "\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建卷积模型\n",
    "\n",
    "实现函数 `conv_net`， 创建卷积神经网络模型。该函数传入一批图片 `x`，并输出对数（logits）。使用你在上方创建的层创建此模型：\n",
    "\n",
    "* 应用 1、2 或 3 个卷积和最大池化层（Convolution and Max Pool layers）\n",
    "* 应用一个扁平层（Flatten Layer）\n",
    "* 应用 1、2 或 3 个完全连接层（Fully Connected Layers）\n",
    "* 应用一个输出层（Output Layer）\n",
    "* 返回输出\n",
    "* 使用 `keep_prob` 向模型中的一个或多个层应用 [TensorFlow 的 Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "def repeat_conv2d_maxpool(layer_num, x_tensor, conv_num_outputs, conv_ksize=(3,3), conv_strides=(1,1), pool_ksize=(2,2), pool_strides=(2,2), scope=\"conv2d_maxpool\"):     \n",
    "        x = x_tensor\n",
    "        \n",
    "        for i in range(0, layer_num - 1):\n",
    "            x = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, (0,0), pool_strides, \"{}_{}\".format(scope, i))\n",
    "        x = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides, \"{}_{}\".format(scope, layer_num - 1))\n",
    "        return x\n",
    "\n",
    "scope_reused = False\n",
    "    \n",
    "def conv_net(x, keep_prob, bn_is_training=tf.constant(True)):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    x_tensor = x\n",
    "    # vgg-16 卷积、池化层参数\n",
    "    conv2d_pool_params = [\n",
    "        (2, 64),\n",
    "        (2, 128),\n",
    "        (3, 256),\n",
    "        (3, 512),\n",
    "        (3, 512),\n",
    "    ]\n",
    "\n",
    "    for i, params in enumerate(conv2d_pool_params):\n",
    "            x_tensor = repeat_conv2d_maxpool(params[0], x_tensor, params[1], scope=\"conv_{}_{}\".format(i, params[1]))\n",
    "#             x_tensor = tf.nn.dropout(x_tensor, keep_prob)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    x_tensor = flatten(x_tensor)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    x_tensor = fully_conn(x_tensor, 4096, \"fc1\")\n",
    "    \n",
    "    x_tensor = tf.nn.dropout(x_tensor, keep_prob)\n",
    "    \n",
    "    x_tensor = fully_conn(x_tensor, 4096, \"fc2\")\n",
    "    \n",
    "    x_tensor = tf.nn.dropout(x_tensor, keep_prob)\n",
    "    \n",
    "    x_tensor = fully_conn(x_tensor, 1000, \"fc3\") \n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return output(x_tensor, 10)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "# batch normal flag\n",
    "bn_is_training = tf.placeholder(tf.bool, name=\"bn_is_training\")\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob, bn_is_training)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tf.get_variable_scope().reuse_variables()\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练神经网络\n",
    "\n",
    "### 单次优化\n",
    "\n",
    "实现函数 `train_neural_network` 以进行单次优化（single optimization）。该优化应该使用 `optimizer` 优化 `session`，其中 `feed_dict` 具有以下参数：\n",
    "\n",
    "* `x` 表示图片输入\n",
    "* `y` 表示标签\n",
    "* `keep_prob` 表示丢弃的保留率\n",
    "\n",
    "每个部分都会调用该函数，所以 `tf.global_variables_initializer()` 已经被调用。\n",
    "\n",
    "注意：不需要返回任何内容。该函数只是用来优化神经网络。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch,\n",
    "                         is_training=True):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    keep_prob_value = keep_probability[0] if hasattr(keep_probability, \"__getitem__\") else keep_probability\n",
    "    session.run(optimizer, feed_dict={x:feature_batch, y:label_batch, keep_prob:keep_prob_value, \n",
    "                                      bn_is_training: is_training})\n",
    "    \n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示数据\n",
    "\n",
    "实现函数 `print_stats` 以输出损失和验证准确率。使用全局变量 `valid_features` 和 `valid_labels` 计算验证准确率。使用保留率 `1.0` 计算损失和验证准确率（loss and validation accuracy）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    global valid_features\n",
    "    global valid_labels\n",
    "    \n",
    "    loss = session.run(cost, feed_dict={x:feature_batch, y:label_batch, keep_prob:1.0, bn_is_training: True})\n",
    "    acc = session.run(accuracy, feed_dict={x:feature_batch, y:label_batch, keep_prob:1.0, bn_is_training: True})\n",
    "    \n",
    "    valid_loss = session.run(cost, feed_dict={x:valid_features, y:valid_labels, keep_prob:1.0, bn_is_training: False})\n",
    "    valid_acc = session.run(accuracy, feed_dict={x:valid_features, y:valid_labels, keep_prob:1.0, bn_is_training:False})\n",
    "    \n",
    "    print(\"loss: {:.4f}, acc: {:.4f}, valid_loss: {:.4f}, valid_acc: {:.4f}, \".format(loss, acc, valid_loss, valid_acc), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数\n",
    "\n",
    "调试以下超参数：\n",
    "* 设置 `epochs` 表示神经网络停止学习或开始过拟合的迭代次数\n",
    "* 设置 `batch_size`，表示机器内存允许的部分最大体积。大部分人设为以下常见内存大小：\n",
    "\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* 设置 `keep_probability` 表示使用丢弃时保留节点的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 1024\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在单个 CIFAR-10 部分上训练\n",
    "\n",
    "我们先用单个部分，而不是用所有的 CIFAR-10 批次训练神经网络。这样可以节省时间，并对模型进行迭代，以提高准确率。最终验证准确率达到 50% 或以上之后，在下一部分对所有数据运行模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 2.2608, acc: 0.1361, valid_loss: 2.2657, valid_acc: 0.1346, [02-22 15:35:07->02-22 15:35:14], cost time: 6.88 s\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 2.2392, acc: 0.1448, valid_loss: 2.2398, valid_acc: 0.1396, [02-22 15:35:14->02-22 15:35:19], cost time: 5.20 s\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 2.2020, acc: 0.1696, valid_loss: 2.2095, valid_acc: 0.1564, [02-22 15:35:19->02-22 15:35:24], cost time: 5.22 s\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 2.1436, acc: 0.1572, valid_loss: 2.1486, valid_acc: 0.1590, [02-22 15:35:24->02-22 15:35:30], cost time: 5.23 s\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 2.0845, acc: 0.1696, valid_loss: 2.0712, valid_acc: 0.1578, [02-22 15:35:30->02-22 15:35:35], cost time: 5.25 s\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 2.0412, acc: 0.1658, valid_loss: 2.0397, valid_acc: 0.1736, [02-22 15:35:35->02-22 15:35:40], cost time: 5.27 s\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 2.0018, acc: 0.1733, valid_loss: 1.9984, valid_acc: 0.1822, [02-22 15:35:40->02-22 15:35:45], cost time: 5.25 s\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 1.9240, acc: 0.2141, valid_loss: 1.9264, valid_acc: 0.1994, [02-22 15:35:45->02-22 15:35:51], cost time: 5.27 s\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 1.8423, acc: 0.2005, valid_loss: 1.8789, valid_acc: 0.2116, [02-22 15:35:51->02-22 15:35:56], cost time: 5.29 s\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 1.7916, acc: 0.2537, valid_loss: 1.8451, valid_acc: 0.2402, [02-22 15:35:56->02-22 15:36:01], cost time: 5.30 s\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 1.7612, acc: 0.2512, valid_loss: 1.8166, valid_acc: 0.2538, [02-22 15:36:01->02-22 15:36:07], cost time: 5.30 s\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 1.7061, acc: 0.2871, valid_loss: 1.7660, valid_acc: 0.2810, [02-22 15:36:07->02-22 15:36:12], cost time: 5.32 s\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 1.6488, acc: 0.3342, valid_loss: 1.7302, valid_acc: 0.2960, [02-22 15:36:12->02-22 15:36:17], cost time: 5.31 s\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 1.6504, acc: 0.3292, valid_loss: 1.7347, valid_acc: 0.3050, [02-22 15:36:17->02-22 15:36:23], cost time: 5.31 s\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 1.5786, acc: 0.3453, valid_loss: 1.7032, valid_acc: 0.3212, [02-22 15:36:23->02-22 15:36:28], cost time: 5.33 s\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 1.6586, acc: 0.3354, valid_loss: 1.8171, valid_acc: 0.2946, [02-22 15:36:28->02-22 15:36:33], cost time: 5.32 s\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 1.4736, acc: 0.4035, valid_loss: 1.6583, valid_acc: 0.3442, [02-22 15:36:33->02-22 15:36:39], cost time: 5.34 s\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss: 1.4044, acc: 0.4220, valid_loss: 1.6214, valid_acc: 0.3636, [02-22 15:36:39->02-22 15:36:44], cost time: 5.34 s\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 1.3546, acc: 0.4257, valid_loss: 1.5495, valid_acc: 0.3758, [02-22 15:36:44->02-22 15:36:49], cost time: 5.35 s\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 1.3942, acc: 0.4245, valid_loss: 1.5683, valid_acc: 0.3662, [02-22 15:36:49->02-22 15:36:55], cost time: 5.48 s\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 1.3209, acc: 0.4493, valid_loss: 1.6012, valid_acc: 0.3488, [02-22 15:36:55->02-22 15:37:00], cost time: 5.41 s\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 1.3079, acc: 0.4517, valid_loss: 1.5788, valid_acc: 0.3790, [02-22 15:37:00->02-22 15:37:06], cost time: 5.52 s\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 1.2645, acc: 0.4864, valid_loss: 1.5742, valid_acc: 0.4070, [02-22 15:37:06->02-22 15:37:11], cost time: 5.52 s\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 1.2391, acc: 0.4740, valid_loss: 1.5421, valid_acc: 0.4010, [02-22 15:37:11->02-22 15:37:17], cost time: 5.57 s\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 1.1350, acc: 0.5223, valid_loss: 1.5356, valid_acc: 0.4252, [02-22 15:37:17->02-22 15:37:22], cost time: 5.48 s\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 1.0810, acc: 0.5347, valid_loss: 1.5058, valid_acc: 0.4164, [02-22 15:37:22->02-22 15:37:28], cost time: 5.46 s\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 1.0758, acc: 0.5347, valid_loss: 1.4988, valid_acc: 0.4248, [02-22 15:37:28->02-22 15:37:33], cost time: 5.39 s\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 1.0216, acc: 0.5718, valid_loss: 1.5401, valid_acc: 0.4286, [02-22 15:37:33->02-22 15:37:39], cost time: 5.46 s\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 0.9619, acc: 0.5941, valid_loss: 1.5039, valid_acc: 0.4360, [02-22 15:37:39->02-22 15:37:44], cost time: 5.42 s\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 0.9403, acc: 0.5780, valid_loss: 1.4233, valid_acc: 0.4608, [02-22 15:37:44->02-22 15:37:49], cost time: 5.39 s\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 0.8982, acc: 0.5891, valid_loss: 1.5903, valid_acc: 0.4520, [02-22 15:37:49->02-22 15:37:55], cost time: 5.54 s\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 0.8992, acc: 0.6077, valid_loss: 1.4825, valid_acc: 0.4518, [02-22 15:37:55->02-22 15:38:00], cost time: 5.44 s\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 0.9059, acc: 0.6139, valid_loss: 1.6112, valid_acc: 0.4406, [02-22 15:38:00->02-22 15:38:06], cost time: 5.53 s\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 0.8521, acc: 0.6238, valid_loss: 1.5338, valid_acc: 0.4680, [02-22 15:38:06->02-22 15:38:11], cost time: 5.44 s\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 0.7896, acc: 0.6856, valid_loss: 1.5117, valid_acc: 0.4784, [02-22 15:38:11->02-22 15:38:17], cost time: 5.50 s\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 0.6887, acc: 0.7005, valid_loss: 1.5886, valid_acc: 0.5018, [02-22 15:38:17->02-22 15:38:22], cost time: 5.72 s\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 0.7070, acc: 0.6993, valid_loss: 1.5596, valid_acc: 0.5014, [02-22 15:38:22->02-22 15:38:28], cost time: 5.47 s\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 0.6355, acc: 0.7599, valid_loss: 1.6470, valid_acc: 0.4974, [02-22 15:38:28->02-22 15:38:33], cost time: 5.52 s\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 0.6372, acc: 0.7488, valid_loss: 1.6514, valid_acc: 0.4928, [02-22 15:38:33->02-22 15:38:39], cost time: 5.55 s\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 0.6409, acc: 0.7351, valid_loss: 1.5970, valid_acc: 0.5030, [02-22 15:38:39->02-22 15:38:45], cost time: 5.52 s\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 0.6026, acc: 0.7661, valid_loss: 1.5353, valid_acc: 0.5258, [02-22 15:38:45->02-22 15:38:50], cost time: 5.54 s\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 0.4511, acc: 0.8144, valid_loss: 1.6515, valid_acc: 0.5264, [02-22 15:38:50->02-22 15:38:56], cost time: 5.53 s\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 0.4773, acc: 0.8106, valid_loss: 1.7258, valid_acc: 0.5188, [02-22 15:38:56->02-22 15:39:01], cost time: 5.51 s\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 0.4146, acc: 0.8428, valid_loss: 1.7081, valid_acc: 0.5474, [02-22 15:39:01->02-22 15:39:07], cost time: 5.50 s\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 0.3911, acc: 0.8453, valid_loss: 1.8530, valid_acc: 0.5314, [02-22 15:39:07->02-22 15:39:12], cost time: 5.49 s\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 0.3697, acc: 0.8577, valid_loss: 1.8958, valid_acc: 0.5378, [02-22 15:39:12->02-22 15:39:18], cost time: 5.58 s\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 0.2969, acc: 0.8985, valid_loss: 1.7301, valid_acc: 0.5546, [02-22 15:39:18->02-22 15:39:23], cost time: 5.53 s\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 0.2773, acc: 0.9183, valid_loss: 1.7900, valid_acc: 0.5682, [02-22 15:39:23->02-22 15:39:29], cost time: 5.48 s\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 0.2704, acc: 0.9171, valid_loss: 1.8569, valid_acc: 0.5606, [02-22 15:39:29->02-22 15:39:34], cost time: 5.60 s\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 0.2252, acc: 0.9270, valid_loss: 1.9213, valid_acc: 0.5700, [02-22 15:39:34->02-22 15:39:40], cost time: 5.51 s\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 0.2634, acc: 0.9084, valid_loss: 2.0096, valid_acc: 0.5564, [02-22 15:39:40->02-22 15:39:45], cost time: 5.44 s\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 0.1290, acc: 0.9653, valid_loss: 1.7666, valid_acc: 0.5952, [02-22 15:39:45->02-22 15:39:51], cost time: 5.47 s\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 0.1374, acc: 0.9653, valid_loss: 2.0826, valid_acc: 0.5766, [02-22 15:39:51->02-22 15:39:56], cost time: 5.52 s\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 0.0959, acc: 0.9678, valid_loss: 2.0638, valid_acc: 0.5834, [02-22 15:39:56->02-22 15:40:02], cost time: 5.53 s\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 0.0714, acc: 0.9777, valid_loss: 2.0103, valid_acc: 0.5978, [02-22 15:40:02->02-22 15:40:07], cost time: 5.53 s\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 0.0699, acc: 0.9827, valid_loss: 2.0464, valid_acc: 0.5868, [02-22 15:40:07->02-22 15:40:13], cost time: 5.45 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57, CIFAR-10 Batch 1:  loss: 0.0914, acc: 0.9777, valid_loss: 2.0012, valid_acc: 0.5968, [02-22 15:40:13->02-22 15:40:18], cost time: 5.53 s\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 0.0594, acc: 0.9876, valid_loss: 2.2381, valid_acc: 0.5972, [02-22 15:40:18->02-22 15:40:24], cost time: 5.41 s\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 0.0485, acc: 0.9889, valid_loss: 2.3246, valid_acc: 0.5984, [02-22 15:40:24->02-22 15:40:29], cost time: 5.53 s\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 0.0614, acc: 0.9827, valid_loss: 2.3717, valid_acc: 0.6000, [02-22 15:40:29->02-22 15:40:35], cost time: 5.39 s\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 0.0626, acc: 0.9814, valid_loss: 2.1411, valid_acc: 0.5932, [02-22 15:40:35->02-22 15:40:40], cost time: 5.53 s\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 0.0581, acc: 0.9814, valid_loss: 2.1403, valid_acc: 0.6284, [02-22 15:40:40->02-22 15:40:46], cost time: 5.45 s\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 0.0313, acc: 0.9926, valid_loss: 2.3605, valid_acc: 0.6108, [02-22 15:40:46->02-22 15:40:51], cost time: 5.54 s\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 0.0236, acc: 0.9963, valid_loss: 2.3169, valid_acc: 0.6098, [02-22 15:40:51->02-22 15:40:57], cost time: 5.45 s\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 0.0180, acc: 0.9975, valid_loss: 2.2212, valid_acc: 0.6250, [02-22 15:40:57->02-22 15:41:02], cost time: 5.41 s\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 0.0520, acc: 0.9889, valid_loss: 2.2387, valid_acc: 0.6134, [02-22 15:41:02->02-22 15:41:07], cost time: 5.52 s\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 0.0467, acc: 0.9876, valid_loss: 2.1626, valid_acc: 0.6222, [02-22 15:41:07->02-22 15:41:13], cost time: 5.44 s\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 0.0625, acc: 0.9827, valid_loss: 2.5375, valid_acc: 0.5898, [02-22 15:41:13->02-22 15:41:18], cost time: 5.42 s\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 0.0770, acc: 0.9802, valid_loss: 2.1899, valid_acc: 0.6048, [02-22 15:41:18->02-22 15:41:24], cost time: 5.47 s\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 0.0502, acc: 0.9876, valid_loss: 2.1471, valid_acc: 0.6278, [02-22 15:41:24->02-22 15:41:29], cost time: 5.50 s\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 0.0331, acc: 0.9938, valid_loss: 2.3679, valid_acc: 0.6300, [02-22 15:41:29->02-22 15:41:35], cost time: 5.42 s\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 0.0180, acc: 0.9963, valid_loss: 2.5526, valid_acc: 0.6302, [02-22 15:41:35->02-22 15:41:40], cost time: 5.51 s\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 0.0136, acc: 0.9963, valid_loss: 2.5911, valid_acc: 0.6362, [02-22 15:41:40->02-22 15:41:46], cost time: 5.47 s\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 0.0072, acc: 0.9988, valid_loss: 2.6471, valid_acc: 0.6332, [02-22 15:41:46->02-22 15:41:51], cost time: 5.46 s\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 0.0107, acc: 0.9963, valid_loss: 2.6098, valid_acc: 0.6378, [02-22 15:41:51->02-22 15:41:57], cost time: 5.54 s\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 0.0035, acc: 1.0000, valid_loss: 2.5443, valid_acc: 0.6434, [02-22 15:41:57->02-22 15:42:02], cost time: 5.53 s\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 0.0060, acc: 0.9988, valid_loss: 2.6021, valid_acc: 0.6410, [02-22 15:42:02->02-22 15:42:08], cost time: 5.47 s\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 0.0033, acc: 1.0000, valid_loss: 2.7033, valid_acc: 0.6378, [02-22 15:42:08->02-22 15:42:13], cost time: 5.53 s\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 0.0027, acc: 1.0000, valid_loss: 2.7089, valid_acc: 0.6414, [02-22 15:42:13->02-22 15:42:19], cost time: 5.50 s\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 0.0014, acc: 1.0000, valid_loss: 2.9272, valid_acc: 0.6406, [02-22 15:42:19->02-22 15:42:24], cost time: 5.42 s\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss: 0.0037, acc: 0.9988, valid_loss: 2.9706, valid_acc: 0.6322, [02-22 15:42:24->02-22 15:42:30], cost time: 5.45 s\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss: 0.0019, acc: 1.0000, valid_loss: 3.0905, valid_acc: 0.6364, [02-22 15:42:30->02-22 15:42:35], cost time: 5.54 s\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss: 0.0020, acc: 1.0000, valid_loss: 2.9936, valid_acc: 0.6400, [02-22 15:42:35->02-22 15:42:41], cost time: 5.46 s\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss: 0.0014, acc: 1.0000, valid_loss: 3.1610, valid_acc: 0.6386, [02-22 15:42:41->02-22 15:42:46], cost time: 5.58 s\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss: 0.0010, acc: 1.0000, valid_loss: 3.0731, valid_acc: 0.6336, [02-22 15:42:46->02-22 15:42:52], cost time: 5.44 s\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss: 0.0087, acc: 0.9975, valid_loss: 3.0945, valid_acc: 0.6252, [02-22 15:42:52->02-22 15:42:57], cost time: 5.42 s\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss: 0.0093, acc: 0.9975, valid_loss: 2.6631, valid_acc: 0.6380, [02-22 15:42:57->02-22 15:43:03], cost time: 5.43 s\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss: 0.0088, acc: 0.9988, valid_loss: 2.5573, valid_acc: 0.6258, [02-22 15:43:03->02-22 15:43:08], cost time: 5.43 s\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss: 0.0075, acc: 1.0000, valid_loss: 2.4182, valid_acc: 0.6476, [02-22 15:43:08->02-22 15:43:13], cost time: 5.54 s\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss: 0.0036, acc: 1.0000, valid_loss: 2.5701, valid_acc: 0.6508, [02-22 15:43:13->02-22 15:43:19], cost time: 5.50 s\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss: 0.0026, acc: 1.0000, valid_loss: 2.7764, valid_acc: 0.6418, [02-22 15:43:19->02-22 15:43:24], cost time: 5.43 s\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss: 0.0081, acc: 0.9988, valid_loss: 3.0505, valid_acc: 0.6424, [02-22 15:43:24->02-22 15:43:30], cost time: 5.44 s\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss: 0.0026, acc: 1.0000, valid_loss: 3.0996, valid_acc: 0.6420, [02-22 15:43:30->02-22 15:43:35], cost time: 5.52 s\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss: 0.0003, acc: 1.0000, valid_loss: 3.1554, valid_acc: 0.6450, [02-22 15:43:35->02-22 15:43:41], cost time: 5.52 s\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss: 0.0018, acc: 1.0000, valid_loss: 3.2119, valid_acc: 0.6456, [02-22 15:43:41->02-22 15:43:46], cost time: 5.55 s\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss: 0.0023, acc: 1.0000, valid_loss: 3.2166, valid_acc: 0.6440, [02-22 15:43:46->02-22 15:43:52], cost time: 5.51 s\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss: 0.0056, acc: 0.9988, valid_loss: 3.3599, valid_acc: 0.6422, [02-22 15:43:52->02-22 15:43:57], cost time: 5.46 s\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss: 0.0014, acc: 1.0000, valid_loss: 3.1152, valid_acc: 0.6398, [02-22 15:43:57->02-22 15:44:03], cost time: 5.48 s\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss: 0.0036, acc: 1.0000, valid_loss: 2.8784, valid_acc: 0.6330, [02-22 15:44:03->02-22 15:44:08], cost time: 5.47 s\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss: 0.0117, acc: 0.9988, valid_loss: 2.4386, valid_acc: 0.6392, [02-22 15:44:08->02-22 15:44:14], cost time: 5.55 s\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss: 0.0035, acc: 1.0000, valid_loss: 2.5428, valid_acc: 0.6464, [02-22 15:44:14->02-22 15:44:19], cost time: 5.53 s\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss: 0.0094, acc: 0.9975, valid_loss: 2.6643, valid_acc: 0.6410, [02-22 15:44:19->02-22 15:44:25], cost time: 5.63 s\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss: 0.0026, acc: 1.0000, valid_loss: 2.7518, valid_acc: 0.6450, [02-22 15:44:25->02-22 15:44:30], cost time: 5.39 s\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss: 0.0034, acc: 1.0000, valid_loss: 2.7406, valid_acc: 0.6478, [02-22 15:44:30->02-22 15:44:36], cost time: 5.39 s\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss: 0.0077, acc: 0.9975, valid_loss: 2.8921, valid_acc: 0.6390, [02-22 15:44:36->02-22 15:44:41], cost time: 5.64 s\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss: 0.0103, acc: 0.9975, valid_loss: 2.5876, valid_acc: 0.6446, [02-22 15:44:41->02-22 15:44:47], cost time: 5.56 s\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss: 0.0069, acc: 0.9988, valid_loss: 2.6767, valid_acc: 0.6330, [02-22 15:44:47->02-22 15:44:53], cost time: 5.48 s\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss: 0.0045, acc: 0.9988, valid_loss: 2.7348, valid_acc: 0.6482, [02-22 15:44:53->02-22 15:44:58], cost time: 5.63 s\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss: 0.0018, acc: 1.0000, valid_loss: 2.7197, valid_acc: 0.6558, [02-22 15:44:58->02-22 15:45:04], cost time: 5.45 s\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss: 0.0104, acc: 0.9988, valid_loss: 2.8436, valid_acc: 0.6468, [02-22 15:45:04->02-22 15:45:09], cost time: 5.53 s\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss: 0.0190, acc: 0.9963, valid_loss: 2.7956, valid_acc: 0.6498, [02-22 15:45:09->02-22 15:45:15], cost time: 5.41 s\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss: 0.0109, acc: 0.9963, valid_loss: 2.5436, valid_acc: 0.6444, [02-22 15:45:15->02-22 15:45:20], cost time: 5.58 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113, CIFAR-10 Batch 1:  loss: 0.0127, acc: 0.9975, valid_loss: 2.4341, valid_acc: 0.6448, [02-22 15:45:20->02-22 15:45:26], cost time: 5.55 s\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss: 0.0096, acc: 0.9975, valid_loss: 2.4713, valid_acc: 0.6524, [02-22 15:45:26->02-22 15:45:31], cost time: 5.39 s\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss: 0.0024, acc: 1.0000, valid_loss: 2.5477, valid_acc: 0.6530, [02-22 15:45:31->02-22 15:45:36], cost time: 5.39 s\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss: 0.0029, acc: 1.0000, valid_loss: 2.7660, valid_acc: 0.6556, [02-22 15:45:36->02-22 15:45:42], cost time: 5.57 s\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss: 0.0017, acc: 1.0000, valid_loss: 2.8385, valid_acc: 0.6486, [02-22 15:45:42->02-22 15:45:48], cost time: 5.51 s\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss: 0.0032, acc: 1.0000, valid_loss: 2.5978, valid_acc: 0.6456, [02-22 15:45:48->02-22 15:45:53], cost time: 5.53 s\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss: 0.0060, acc: 0.9988, valid_loss: 2.5335, valid_acc: 0.6602, [02-22 15:45:53->02-22 15:45:59], cost time: 5.64 s\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss: 0.0024, acc: 1.0000, valid_loss: 2.7390, valid_acc: 0.6606, [02-22 15:45:59->02-22 15:46:04], cost time: 5.52 s\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss: 0.0007, acc: 1.0000, valid_loss: 2.8827, valid_acc: 0.6608, [02-22 15:46:04->02-22 15:46:10], cost time: 5.65 s\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss: 0.0034, acc: 1.0000, valid_loss: 3.1116, valid_acc: 0.6502, [02-22 15:46:10->02-22 15:46:15], cost time: 5.42 s\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss: 0.0095, acc: 0.9975, valid_loss: 2.9202, valid_acc: 0.6286, [02-22 15:46:15->02-22 15:46:21], cost time: 5.52 s\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss: 0.0063, acc: 1.0000, valid_loss: 2.4040, valid_acc: 0.6594, [02-22 15:46:21->02-22 15:46:26], cost time: 5.45 s\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss: 0.0250, acc: 0.9963, valid_loss: 2.4005, valid_acc: 0.6444, [02-22 15:46:26->02-22 15:46:32], cost time: 5.42 s\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss: 0.0074, acc: 1.0000, valid_loss: 2.4539, valid_acc: 0.6526, [02-22 15:46:32->02-22 15:46:37], cost time: 5.48 s\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss: 0.0046, acc: 0.9988, valid_loss: 2.5663, valid_acc: 0.6536, [02-22 15:46:37->02-22 15:46:43], cost time: 5.53 s\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss: 0.0031, acc: 1.0000, valid_loss: 2.7260, valid_acc: 0.6536, [02-22 15:46:43->02-22 15:46:48], cost time: 5.54 s\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss: 0.0019, acc: 1.0000, valid_loss: 2.7539, valid_acc: 0.6566, [02-22 15:46:48->02-22 15:46:54], cost time: 5.46 s\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss: 0.0018, acc: 1.0000, valid_loss: 2.7807, valid_acc: 0.6582, [02-22 15:46:54->02-22 15:46:59], cost time: 5.56 s\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss: 0.0032, acc: 0.9988, valid_loss: 2.8454, valid_acc: 0.6520, [02-22 15:46:59->02-22 15:47:05], cost time: 5.48 s\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss: 0.0053, acc: 0.9975, valid_loss: 2.8713, valid_acc: 0.6572, [02-22 15:47:05->02-22 15:47:10], cost time: 5.47 s\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss: 0.0012, acc: 1.0000, valid_loss: 2.8871, valid_acc: 0.6580, [02-22 15:47:10->02-22 15:47:16], cost time: 5.53 s\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss: 0.0016, acc: 1.0000, valid_loss: 2.9012, valid_acc: 0.6574, [02-22 15:47:16->02-22 15:47:21], cost time: 5.50 s\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss: 0.0013, acc: 1.0000, valid_loss: 3.1037, valid_acc: 0.6620, [02-22 15:47:21->02-22 15:47:27], cost time: 5.46 s\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss: 0.0009, acc: 1.0000, valid_loss: 3.1858, valid_acc: 0.6526, [02-22 15:47:27->02-22 15:47:32], cost time: 5.40 s\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss: 0.0175, acc: 0.9950, valid_loss: 3.1124, valid_acc: 0.6374, [02-22 15:47:32->02-22 15:47:38], cost time: 5.45 s\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss: 0.0053, acc: 1.0000, valid_loss: 2.8632, valid_acc: 0.6512, [02-22 15:47:38->02-22 15:47:43], cost time: 5.43 s\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss: 0.0041, acc: 1.0000, valid_loss: 2.6059, valid_acc: 0.6564, [02-22 15:47:43->02-22 15:47:49], cost time: 5.54 s\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss: 0.0037, acc: 1.0000, valid_loss: 2.4589, valid_acc: 0.6540, [02-22 15:47:49->02-22 15:47:54], cost time: 5.58 s\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss: 0.0055, acc: 0.9988, valid_loss: 2.5457, valid_acc: 0.6510, [02-22 15:47:54->02-22 15:48:00], cost time: 5.48 s\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss: 0.0033, acc: 0.9988, valid_loss: 2.5888, valid_acc: 0.6546, [02-22 15:48:00->02-22 15:48:05], cost time: 5.61 s\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss: 0.0014, acc: 1.0000, valid_loss: 2.7607, valid_acc: 0.6514, [02-22 15:48:05->02-22 15:48:11], cost time: 5.49 s\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss: 0.0027, acc: 1.0000, valid_loss: 2.6659, valid_acc: 0.6580, [02-22 15:48:11->02-22 15:48:16], cost time: 5.44 s\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss: 0.0076, acc: 0.9988, valid_loss: 2.6614, valid_acc: 0.6498, [02-22 15:48:16->02-22 15:48:22], cost time: 5.46 s\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss: 0.0025, acc: 1.0000, valid_loss: 2.7479, valid_acc: 0.6494, [02-22 15:48:22->02-22 15:48:27], cost time: 5.40 s\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss: 0.0019, acc: 1.0000, valid_loss: 2.7239, valid_acc: 0.6526, [02-22 15:48:27->02-22 15:48:33], cost time: 5.53 s\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss: 0.0076, acc: 0.9975, valid_loss: 2.4754, valid_acc: 0.6572, [02-22 15:48:33->02-22 15:48:38], cost time: 5.53 s\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss: 0.0037, acc: 1.0000, valid_loss: 2.3411, valid_acc: 0.6570, [02-22 15:48:38->02-22 15:48:44], cost time: 5.54 s\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss: 0.0032, acc: 0.9988, valid_loss: 2.4057, valid_acc: 0.6612, [02-22 15:48:44->02-22 15:48:49], cost time: 5.43 s\n",
      "Epoch 151, CIFAR-10 Batch 1:  loss: 0.0048, acc: 0.9988, valid_loss: 2.7931, valid_acc: 0.6484, [02-22 15:48:49->02-22 15:48:55], cost time: 5.53 s\n",
      "Epoch 152, CIFAR-10 Batch 1:  loss: 0.0085, acc: 0.9988, valid_loss: 2.7550, valid_acc: 0.6592, [02-22 15:48:55->02-22 15:49:00], cost time: 5.43 s\n",
      "Epoch 153, CIFAR-10 Batch 1:  loss: 0.0011, acc: 1.0000, valid_loss: 2.7318, valid_acc: 0.6614, [02-22 15:49:00->02-22 15:49:06], cost time: 5.52 s\n",
      "Epoch 154, CIFAR-10 Batch 1:  loss: 0.0013, acc: 1.0000, valid_loss: 2.6883, valid_acc: 0.6582, [02-22 15:49:06->02-22 15:49:11], cost time: 5.48 s\n",
      "Epoch 155, CIFAR-10 Batch 1:  loss: 0.0010, acc: 1.0000, valid_loss: 2.6702, valid_acc: 0.6682, [02-22 15:49:11->02-22 15:49:16], cost time: 5.49 s\n",
      "Epoch 156, CIFAR-10 Batch 1:  loss: 0.0010, acc: 1.0000, valid_loss: 2.7483, valid_acc: 0.6652, [02-22 15:49:16->02-22 15:49:22], cost time: 5.49 s\n",
      "Epoch 157, CIFAR-10 Batch 1:  loss: 0.0018, acc: 0.9988, valid_loss: 2.8794, valid_acc: 0.6618, [02-22 15:49:22->02-22 15:49:28], cost time: 5.53 s\n",
      "Epoch 158, CIFAR-10 Batch 1:  loss: 0.0019, acc: 1.0000, valid_loss: 2.9442, valid_acc: 0.6510, [02-22 15:49:28->02-22 15:49:33], cost time: 5.47 s\n",
      "Epoch 159, CIFAR-10 Batch 1:  loss: 0.0010, acc: 1.0000, valid_loss: 2.8048, valid_acc: 0.6672, [02-22 15:49:33->02-22 15:49:39], cost time: 5.62 s\n",
      "Epoch 160, CIFAR-10 Batch 1:  loss: 0.0008, acc: 1.0000, valid_loss: 2.7784, valid_acc: 0.6672, [02-22 15:49:39->02-22 15:49:44], cost time: 5.59 s\n",
      "Epoch 161, CIFAR-10 Batch 1:  loss: 0.0008, acc: 1.0000, valid_loss: 2.7891, valid_acc: 0.6698, [02-22 15:49:44->02-22 15:49:50], cost time: 5.39 s\n",
      "Epoch 162, CIFAR-10 Batch 1:  loss: 0.0013, acc: 1.0000, valid_loss: 2.8623, valid_acc: 0.6694, [02-22 15:49:50->02-22 15:49:55], cost time: 5.59 s\n",
      "Epoch 163, CIFAR-10 Batch 1:  loss: 0.0007, acc: 1.0000, valid_loss: 2.8182, valid_acc: 0.6668, [02-22 15:49:55->02-22 15:50:01], cost time: 5.72 s\n",
      "Epoch 164, CIFAR-10 Batch 1:  loss: 0.0048, acc: 0.9988, valid_loss: 2.7692, valid_acc: 0.6622, [02-22 15:50:01->02-22 15:50:06], cost time: 5.44 s\n",
      "Epoch 165, CIFAR-10 Batch 1:  loss: 0.0027, acc: 1.0000, valid_loss: 2.7127, valid_acc: 0.6598, [02-22 15:50:06->02-22 15:50:12], cost time: 5.47 s\n",
      "Epoch 166, CIFAR-10 Batch 1:  loss: 0.0020, acc: 1.0000, valid_loss: 2.6857, valid_acc: 0.6636, [02-22 15:50:12->02-22 15:50:17], cost time: 5.42 s\n",
      "Epoch 167, CIFAR-10 Batch 1:  loss: 0.0026, acc: 1.0000, valid_loss: 2.6759, valid_acc: 0.6578, [02-22 15:50:17->02-22 15:50:23], cost time: 5.70 s\n",
      "Epoch 168, CIFAR-10 Batch 1:  loss: 0.0023, acc: 1.0000, valid_loss: 2.7131, valid_acc: 0.6568, [02-22 15:50:23->02-22 15:50:28], cost time: 5.54 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169, CIFAR-10 Batch 1:  loss: 0.0007, acc: 1.0000, valid_loss: 2.8921, valid_acc: 0.6608, [02-22 15:50:28->02-22 15:50:34], cost time: 5.46 s\n",
      "Epoch 170, CIFAR-10 Batch 1:  loss: 0.0008, acc: 1.0000, valid_loss: 2.8094, valid_acc: 0.6722, [02-22 15:50:34->02-22 15:50:39], cost time: 5.48 s\n",
      "Epoch 171, CIFAR-10 Batch 1:  loss: 0.0009, acc: 1.0000, valid_loss: 2.8032, valid_acc: 0.6682, [02-22 15:50:39->02-22 15:50:45], cost time: 5.47 s\n",
      "Epoch 172, CIFAR-10 Batch 1:  loss: 0.0072, acc: 0.9988, valid_loss: 2.8107, valid_acc: 0.6656, [02-22 15:50:45->02-22 15:50:50], cost time: 5.60 s\n",
      "Epoch 173, CIFAR-10 Batch 1:  loss: 0.0026, acc: 1.0000, valid_loss: 2.9592, valid_acc: 0.6602, [02-22 15:50:50->02-22 15:50:56], cost time: 5.54 s\n",
      "Epoch 174, CIFAR-10 Batch 1:  loss: 0.0033, acc: 0.9988, valid_loss: 2.9457, valid_acc: 0.6668, [02-22 15:50:56->02-22 15:51:01], cost time: 5.42 s\n",
      "Epoch 175, CIFAR-10 Batch 1:  loss: 0.0012, acc: 1.0000, valid_loss: 2.8811, valid_acc: 0.6702, [02-22 15:51:01->02-22 15:51:07], cost time: 5.54 s\n",
      "Epoch 176, CIFAR-10 Batch 1:  loss: 0.0007, acc: 1.0000, valid_loss: 2.9896, valid_acc: 0.6654, [02-22 15:51:07->02-22 15:51:12], cost time: 5.51 s\n",
      "Epoch 177, CIFAR-10 Batch 1:  loss: 0.0013, acc: 1.0000, valid_loss: 2.8460, valid_acc: 0.6618, [02-22 15:51:12->02-22 15:51:18], cost time: 5.42 s\n",
      "Epoch 178, CIFAR-10 Batch 1:  loss: 0.0075, acc: 0.9988, valid_loss: 2.6619, valid_acc: 0.6546, [02-22 15:51:18->02-22 15:51:24], cost time: 5.72 s\n",
      "Epoch 179, CIFAR-10 Batch 1:  loss: 0.0065, acc: 0.9975, valid_loss: 2.6192, valid_acc: 0.6582, [02-22 15:51:24->02-22 15:51:29], cost time: 5.59 s\n",
      "Epoch 180, CIFAR-10 Batch 1:  loss: 0.0022, acc: 1.0000, valid_loss: 2.4391, valid_acc: 0.6660, [02-22 15:51:29->02-22 15:51:35], cost time: 5.41 s\n",
      "Epoch 181, CIFAR-10 Batch 1:  loss: 0.0016, acc: 1.0000, valid_loss: 2.7382, valid_acc: 0.6726, [02-22 15:51:35->02-22 15:51:40], cost time: 5.51 s\n",
      "Epoch 182, CIFAR-10 Batch 1:  loss: 0.0006, acc: 1.0000, valid_loss: 2.8968, valid_acc: 0.6722, [02-22 15:51:40->02-22 15:51:46], cost time: 5.45 s\n",
      "Epoch 183, CIFAR-10 Batch 1:  loss: 0.0003, acc: 1.0000, valid_loss: 2.8817, valid_acc: 0.6756, [02-22 15:51:46->02-22 15:51:51], cost time: 5.41 s\n",
      "Epoch 184, CIFAR-10 Batch 1:  loss: 0.0004, acc: 1.0000, valid_loss: 3.0128, valid_acc: 0.6698, [02-22 15:51:51->02-22 15:51:57], cost time: 5.54 s\n",
      "Epoch 185, CIFAR-10 Batch 1:  loss: 0.0004, acc: 1.0000, valid_loss: 3.0732, valid_acc: 0.6716, [02-22 15:51:57->02-22 15:52:02], cost time: 5.51 s\n",
      "Epoch 186, CIFAR-10 Batch 1:  loss: 0.0030, acc: 1.0000, valid_loss: 3.2202, valid_acc: 0.6608, [02-22 15:52:02->02-22 15:52:08], cost time: 5.54 s\n",
      "Epoch 187, CIFAR-10 Batch 1:  loss: 0.0075, acc: 0.9988, valid_loss: 3.0417, valid_acc: 0.6618, [02-22 15:52:08->02-22 15:52:13], cost time: 5.43 s\n",
      "Epoch 188, CIFAR-10 Batch 1:  loss: 0.0067, acc: 0.9988, valid_loss: 2.7092, valid_acc: 0.6540, [02-22 15:52:13->02-22 15:52:18], cost time: 5.46 s\n",
      "Epoch 189, CIFAR-10 Batch 1:  loss: 0.0039, acc: 1.0000, valid_loss: 2.3820, valid_acc: 0.6618, [02-22 15:52:18->02-22 15:52:24], cost time: 5.56 s\n",
      "Epoch 190, CIFAR-10 Batch 1:  loss: 0.0042, acc: 1.0000, valid_loss: 2.4722, valid_acc: 0.6694, [02-22 15:52:24->02-22 15:52:30], cost time: 5.57 s\n",
      "Epoch 191, CIFAR-10 Batch 1:  loss: 0.0028, acc: 1.0000, valid_loss: 2.7425, valid_acc: 0.6618, [02-22 15:52:30->02-22 15:52:35], cost time: 5.57 s\n",
      "Epoch 192, CIFAR-10 Batch 1:  loss: 0.0043, acc: 1.0000, valid_loss: 2.7530, valid_acc: 0.6558, [02-22 15:52:35->02-22 15:52:41], cost time: 5.39 s\n",
      "Epoch 193, CIFAR-10 Batch 1:  loss: 0.0015, acc: 1.0000, valid_loss: 2.5149, valid_acc: 0.6668, [02-22 15:52:41->02-22 15:52:46], cost time: 5.50 s\n",
      "Epoch 194, CIFAR-10 Batch 1:  loss: 0.0040, acc: 0.9988, valid_loss: 2.3763, valid_acc: 0.6752, [02-22 15:52:46->02-22 15:52:52], cost time: 5.56 s\n",
      "Epoch 195, CIFAR-10 Batch 1:  loss: 0.0026, acc: 1.0000, valid_loss: 2.5000, valid_acc: 0.6678, [02-22 15:52:52->02-22 15:52:57], cost time: 5.48 s\n",
      "Epoch 196, CIFAR-10 Batch 1:  loss: 0.0020, acc: 0.9988, valid_loss: 2.5607, valid_acc: 0.6766, [02-22 15:52:57->02-22 15:53:03], cost time: 5.47 s\n",
      "Epoch 197, CIFAR-10 Batch 1:  loss: 0.0016, acc: 1.0000, valid_loss: 2.8146, valid_acc: 0.6712, [02-22 15:53:03->02-22 15:53:08], cost time: 5.45 s\n",
      "Epoch 198, CIFAR-10 Batch 1:  loss: 0.0011, acc: 1.0000, valid_loss: 2.6718, valid_acc: 0.6652, [02-22 15:53:08->02-22 15:53:14], cost time: 5.53 s\n",
      "Epoch 199, CIFAR-10 Batch 1:  loss: 0.0012, acc: 1.0000, valid_loss: 2.7326, valid_acc: 0.6732, [02-22 15:53:14->02-22 15:53:19], cost time: 5.51 s\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss: 0.0006, acc: 1.0000, valid_loss: 2.8142, valid_acc: 0.6644, [02-22 15:53:19->02-22 15:53:24], cost time: 5.42 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        batch_i = 1\n",
    "                \n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "                \n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "        end_time = time.time()\n",
    "        print(\"[{}->{}], cost time: {:.2f} s\".format(time.strftime(\"%m-%d %H:%M:%S\", time.localtime(start_time)),\n",
    "                                                time.strftime(\"%m-%d %H:%M:%S\", time.localtime(end_time)),\n",
    "                                                end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完全训练模型\n",
    "\n",
    "现在，单个 CIFAR-10 部分的准确率已经不错了，试试所有五个部分吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 2.2929, acc: 0.1027, valid_loss: 2.2858, valid_acc: 0.0978, \n",
      "Epoch  1, CIFAR-10 Batch 2:  loss: 2.2986, acc: 0.0978, valid_loss: 2.2909, valid_acc: 0.0996, \n",
      "Epoch  1, CIFAR-10 Batch 3:  loss: 2.2640, acc: 0.1386, valid_loss: 2.2700, valid_acc: 0.1210, \n",
      "Epoch  1, CIFAR-10 Batch 4:  loss: 2.2352, acc: 0.1485, valid_loss: 2.2440, valid_acc: 0.1260, \n",
      "Epoch  1, CIFAR-10 Batch 5:  loss: 2.1833, acc: 0.1522, valid_loss: 2.2052, valid_acc: 0.1462, \n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 2.1337, acc: 0.1807, valid_loss: 2.1330, valid_acc: 0.1674, \n",
      "Epoch  2, CIFAR-10 Batch 2:  loss: 2.0686, acc: 0.1819, valid_loss: 2.0965, valid_acc: 0.1810, \n",
      "Epoch  2, CIFAR-10 Batch 3:  loss: 2.0395, acc: 0.1931, valid_loss: 2.0499, valid_acc: 0.1652, \n",
      "Epoch  2, CIFAR-10 Batch 4:  loss: 1.9736, acc: 0.2030, valid_loss: 1.9777, valid_acc: 0.2006, \n",
      "Epoch  2, CIFAR-10 Batch 5:  loss: 1.9315, acc: 0.2203, valid_loss: 1.9218, valid_acc: 0.2128, \n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 1.8873, acc: 0.2191, valid_loss: 1.8698, valid_acc: 0.2358, \n",
      "Epoch  3, CIFAR-10 Batch 2:  loss: 1.8359, acc: 0.2426, valid_loss: 1.8622, valid_acc: 0.2374, \n",
      "Epoch  3, CIFAR-10 Batch 3:  loss: 1.8051, acc: 0.2698, valid_loss: 1.8377, valid_acc: 0.2378, \n",
      "Epoch  3, CIFAR-10 Batch 4:  loss: 1.8144, acc: 0.2686, valid_loss: 1.8227, valid_acc: 0.2590, \n",
      "Epoch  3, CIFAR-10 Batch 5:  loss: 1.7926, acc: 0.2748, valid_loss: 1.8099, valid_acc: 0.2654, \n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 1.8138, acc: 0.2908, valid_loss: 1.7959, valid_acc: 0.2800, \n",
      "Epoch  4, CIFAR-10 Batch 2:  loss: 1.7828, acc: 0.2785, valid_loss: 1.7885, valid_acc: 0.2770, \n",
      "Epoch  4, CIFAR-10 Batch 3:  loss: 1.6971, acc: 0.3255, valid_loss: 1.7494, valid_acc: 0.3046, \n",
      "Epoch  4, CIFAR-10 Batch 4:  loss: 1.7149, acc: 0.3230, valid_loss: 1.7291, valid_acc: 0.3132, \n",
      "Epoch  4, CIFAR-10 Batch 5:  loss: 1.6502, acc: 0.3304, valid_loss: 1.6856, valid_acc: 0.3264, \n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 1.6491, acc: 0.3144, valid_loss: 1.6882, valid_acc: 0.3096, \n",
      "Epoch  5, CIFAR-10 Batch 2:  loss: 1.6030, acc: 0.3837, valid_loss: 1.6324, valid_acc: 0.3590, \n",
      "Epoch  5, CIFAR-10 Batch 3:  loss: 1.5133, acc: 0.4171, valid_loss: 1.5883, valid_acc: 0.3832, \n",
      "Epoch  5, CIFAR-10 Batch 4:  loss: 1.5228, acc: 0.4431, valid_loss: 1.5524, valid_acc: 0.4010, \n",
      "Epoch  5, CIFAR-10 Batch 5:  loss: 1.4983, acc: 0.4109, valid_loss: 1.5272, valid_acc: 0.4056, \n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 1.5127, acc: 0.4196, valid_loss: 1.5606, valid_acc: 0.4090, \n",
      "Epoch  6, CIFAR-10 Batch 2:  loss: 1.4315, acc: 0.4814, valid_loss: 1.4921, valid_acc: 0.4348, \n",
      "Epoch  6, CIFAR-10 Batch 3:  loss: 1.3391, acc: 0.4950, valid_loss: 1.4116, valid_acc: 0.4588, \n",
      "Epoch  6, CIFAR-10 Batch 4:  loss: 1.3588, acc: 0.4851, valid_loss: 1.4026, valid_acc: 0.4608, \n",
      "Epoch  6, CIFAR-10 Batch 5:  loss: 1.2638, acc: 0.5124, valid_loss: 1.3430, valid_acc: 0.4936, \n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 1.2760, acc: 0.5074, valid_loss: 1.3721, valid_acc: 0.4696, \n",
      "Epoch  7, CIFAR-10 Batch 2:  loss: 1.2514, acc: 0.5520, valid_loss: 1.3220, valid_acc: 0.5058, \n",
      "Epoch  7, CIFAR-10 Batch 3:  loss: 1.1463, acc: 0.5656, valid_loss: 1.2441, valid_acc: 0.5296, \n",
      "Epoch  7, CIFAR-10 Batch 4:  loss: 1.1458, acc: 0.5668, valid_loss: 1.2364, valid_acc: 0.5336, \n",
      "Epoch  7, CIFAR-10 Batch 5:  loss: 1.1032, acc: 0.5767, valid_loss: 1.2270, valid_acc: 0.5366, \n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 1.0761, acc: 0.5990, valid_loss: 1.1643, valid_acc: 0.5750, \n",
      "Epoch  8, CIFAR-10 Batch 2:  loss: 1.0821, acc: 0.6077, valid_loss: 1.1725, valid_acc: 0.5650, \n",
      "Epoch  8, CIFAR-10 Batch 3:  loss: 1.0253, acc: 0.6126, valid_loss: 1.1956, valid_acc: 0.5610, \n",
      "Epoch  8, CIFAR-10 Batch 4:  loss: 1.0298, acc: 0.6337, valid_loss: 1.1201, valid_acc: 0.5842, \n",
      "Epoch  8, CIFAR-10 Batch 5:  loss: 0.9428, acc: 0.6411, valid_loss: 1.1008, valid_acc: 0.6014, \n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 0.9768, acc: 0.6225, valid_loss: 1.0675, valid_acc: 0.6036, \n",
      "Epoch  9, CIFAR-10 Batch 2:  loss: 0.9297, acc: 0.6572, valid_loss: 1.0847, valid_acc: 0.6072, \n",
      "Epoch  9, CIFAR-10 Batch 3:  loss: 0.8664, acc: 0.6634, valid_loss: 1.0613, valid_acc: 0.6144, \n",
      "Epoch  9, CIFAR-10 Batch 4:  loss: 0.9021, acc: 0.6522, valid_loss: 1.0228, valid_acc: 0.6172, \n",
      "Epoch  9, CIFAR-10 Batch 5:  loss: 0.8385, acc: 0.7079, valid_loss: 1.0162, valid_acc: 0.6352, \n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 0.8215, acc: 0.7017, valid_loss: 0.9420, valid_acc: 0.6594, \n",
      "Epoch 10, CIFAR-10 Batch 2:  loss: 0.7750, acc: 0.7005, valid_loss: 0.9548, valid_acc: 0.6528, \n",
      "Epoch 10, CIFAR-10 Batch 3:  loss: 0.7475, acc: 0.7413, valid_loss: 0.9680, valid_acc: 0.6654, \n",
      "Epoch 10, CIFAR-10 Batch 4:  loss: 0.7411, acc: 0.7265, valid_loss: 0.9367, valid_acc: 0.6720, \n",
      "Epoch 10, CIFAR-10 Batch 5:  loss: 0.7186, acc: 0.7376, valid_loss: 0.9112, valid_acc: 0.6794, \n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 0.7468, acc: 0.7203, valid_loss: 0.9195, valid_acc: 0.6746, \n",
      "Epoch 11, CIFAR-10 Batch 2:  loss: 0.7079, acc: 0.7463, valid_loss: 0.8890, valid_acc: 0.6836, \n",
      "Epoch 11, CIFAR-10 Batch 3:  loss: 0.6534, acc: 0.7673, valid_loss: 0.8920, valid_acc: 0.6908, \n",
      "Epoch 11, CIFAR-10 Batch 4:  loss: 0.6252, acc: 0.7847, valid_loss: 0.8878, valid_acc: 0.6874, \n",
      "Epoch 11, CIFAR-10 Batch 5:  loss: 0.6126, acc: 0.7822, valid_loss: 0.8853, valid_acc: 0.7048, \n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 0.6583, acc: 0.7649, valid_loss: 0.8449, valid_acc: 0.7146, \n",
      "Epoch 12, CIFAR-10 Batch 2:  loss: 0.6021, acc: 0.8032, valid_loss: 0.8456, valid_acc: 0.7220, \n",
      "Epoch 12, CIFAR-10 Batch 3:  loss: 0.5229, acc: 0.8379, valid_loss: 0.8281, valid_acc: 0.7326, \n",
      "Epoch 12, CIFAR-10 Batch 4:  loss: 0.5505, acc: 0.8205, valid_loss: 0.8372, valid_acc: 0.7232, \n",
      "Epoch 12, CIFAR-10 Batch 5:  loss: 0.5276, acc: 0.8168, valid_loss: 0.8079, valid_acc: 0.7342, \n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 0.5435, acc: 0.8094, valid_loss: 0.7875, valid_acc: 0.7348, \n",
      "Epoch 13, CIFAR-10 Batch 2:  loss: 0.5011, acc: 0.8280, valid_loss: 0.8017, valid_acc: 0.7524, \n",
      "Epoch 13, CIFAR-10 Batch 3:  loss: 0.4818, acc: 0.8428, valid_loss: 0.7771, valid_acc: 0.7454, \n",
      "Epoch 13, CIFAR-10 Batch 4:  loss: 0.4491, acc: 0.8639, valid_loss: 0.7682, valid_acc: 0.7546, \n",
      "Epoch 13, CIFAR-10 Batch 5:  loss: 0.4085, acc: 0.8651, valid_loss: 0.7334, valid_acc: 0.7670, \n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 0.4848, acc: 0.8366, valid_loss: 0.7506, valid_acc: 0.7546, \n",
      "Epoch 14, CIFAR-10 Batch 2:  loss: 0.4640, acc: 0.8502, valid_loss: 0.7603, valid_acc: 0.7712, \n",
      "Epoch 14, CIFAR-10 Batch 3:  loss: 0.4541, acc: 0.8552, valid_loss: 0.7554, valid_acc: 0.7644, \n",
      "Epoch 14, CIFAR-10 Batch 4:  loss: 0.3265, acc: 0.9047, valid_loss: 0.7475, valid_acc: 0.7716, \n",
      "Epoch 14, CIFAR-10 Batch 5:  loss: 0.3570, acc: 0.8874, valid_loss: 0.7840, valid_acc: 0.7668, \n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 0.4606, acc: 0.8552, valid_loss: 0.7461, valid_acc: 0.7660, \n",
      "Epoch 15, CIFAR-10 Batch 2:  loss: 0.3886, acc: 0.8775, valid_loss: 0.7617, valid_acc: 0.7742, \n",
      "Epoch 15, CIFAR-10 Batch 3:  loss: 0.4172, acc: 0.8540, valid_loss: 0.7287, valid_acc: 0.7722, \n",
      "Epoch 15, CIFAR-10 Batch 4:  loss: 0.3034, acc: 0.9084, valid_loss: 0.7912, valid_acc: 0.7698, \n",
      "Epoch 15, CIFAR-10 Batch 5:  loss: 0.3393, acc: 0.8861, valid_loss: 0.7372, valid_acc: 0.7794, \n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 0.3399, acc: 0.8824, valid_loss: 0.7193, valid_acc: 0.7836, \n",
      "Epoch 16, CIFAR-10 Batch 2:  loss: 0.3466, acc: 0.8812, valid_loss: 0.7757, valid_acc: 0.7804, \n",
      "Epoch 16, CIFAR-10 Batch 3:  loss: 0.3338, acc: 0.8923, valid_loss: 0.6858, valid_acc: 0.7866, \n",
      "Epoch 16, CIFAR-10 Batch 4:  loss: 0.2866, acc: 0.9097, valid_loss: 0.7401, valid_acc: 0.7860, \n",
      "Epoch 16, CIFAR-10 Batch 5:  loss: 0.2845, acc: 0.9097, valid_loss: 0.6832, valid_acc: 0.7926, \n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 0.3269, acc: 0.8936, valid_loss: 0.7791, valid_acc: 0.7786, \n",
      "Epoch 17, CIFAR-10 Batch 2:  loss: 0.3072, acc: 0.9047, valid_loss: 0.7307, valid_acc: 0.7822, \n",
      "Epoch 17, CIFAR-10 Batch 3:  loss: 0.3160, acc: 0.9047, valid_loss: 0.7411, valid_acc: 0.7832, \n",
      "Epoch 17, CIFAR-10 Batch 4:  loss: 0.2759, acc: 0.9109, valid_loss: 0.7190, valid_acc: 0.7892, \n",
      "Epoch 17, CIFAR-10 Batch 5:  loss: 0.2266, acc: 0.9282, valid_loss: 0.6662, valid_acc: 0.8048, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, CIFAR-10 Batch 1:  loss: 0.2484, acc: 0.9307, valid_loss: 0.6790, valid_acc: 0.8038, \n",
      "Epoch 18, CIFAR-10 Batch 2:  loss: 0.2342, acc: 0.9344, valid_loss: 0.7340, valid_acc: 0.7966, \n",
      "Epoch 18, CIFAR-10 Batch 3:  loss: 0.2298, acc: 0.9307, valid_loss: 0.6763, valid_acc: 0.8026, \n",
      "Epoch 18, CIFAR-10 Batch 4:  loss: 0.2091, acc: 0.9295, valid_loss: 0.6722, valid_acc: 0.8074, \n",
      "Epoch 18, CIFAR-10 Batch 5:  loss: 0.1866, acc: 0.9455, valid_loss: 0.6973, valid_acc: 0.8036, \n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 0.1930, acc: 0.9468, valid_loss: 0.6869, valid_acc: 0.8018, \n",
      "Epoch 19, CIFAR-10 Batch 2:  loss: 0.2043, acc: 0.9270, valid_loss: 0.7368, valid_acc: 0.7992, \n",
      "Epoch 19, CIFAR-10 Batch 3:  loss: 0.1952, acc: 0.9356, valid_loss: 0.6708, valid_acc: 0.8128, \n",
      "Epoch 19, CIFAR-10 Batch 4:  loss: 0.1873, acc: 0.9418, valid_loss: 0.6622, valid_acc: 0.8064, \n",
      "Epoch 19, CIFAR-10 Batch 5:  loss: 0.1676, acc: 0.9455, valid_loss: 0.6859, valid_acc: 0.8080, \n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 0.1650, acc: 0.9480, valid_loss: 0.7199, valid_acc: 0.8008, \n",
      "Epoch 20, CIFAR-10 Batch 2:  loss: 0.1957, acc: 0.9418, valid_loss: 0.7141, valid_acc: 0.8008, \n",
      "Epoch 20, CIFAR-10 Batch 3:  loss: 0.1647, acc: 0.9480, valid_loss: 0.7113, valid_acc: 0.8048, \n",
      "Epoch 20, CIFAR-10 Batch 4:  loss: 0.1549, acc: 0.9629, valid_loss: 0.6796, valid_acc: 0.8142, \n",
      "Epoch 20, CIFAR-10 Batch 5:  loss: 0.1347, acc: 0.9542, valid_loss: 0.7136, valid_acc: 0.8102, \n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 0.1753, acc: 0.9406, valid_loss: 0.7415, valid_acc: 0.8082, \n",
      "Epoch 21, CIFAR-10 Batch 2:  loss: 0.1753, acc: 0.9493, valid_loss: 0.7289, valid_acc: 0.8056, \n",
      "Epoch 21, CIFAR-10 Batch 3:  loss: 0.1599, acc: 0.9468, valid_loss: 0.7421, valid_acc: 0.8040, \n",
      "Epoch 21, CIFAR-10 Batch 4:  loss: 0.1567, acc: 0.9493, valid_loss: 0.6841, valid_acc: 0.8132, \n",
      "Epoch 21, CIFAR-10 Batch 5:  loss: 0.1224, acc: 0.9678, valid_loss: 0.7556, valid_acc: 0.8068, \n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 0.1371, acc: 0.9666, valid_loss: 0.7085, valid_acc: 0.8202, \n",
      "Epoch 22, CIFAR-10 Batch 2:  loss: 0.1605, acc: 0.9517, valid_loss: 0.7149, valid_acc: 0.8088, \n",
      "Epoch 22, CIFAR-10 Batch 3:  loss: 0.1622, acc: 0.9554, valid_loss: 0.7313, valid_acc: 0.8106, \n",
      "Epoch 22, CIFAR-10 Batch 4:  loss: 0.0973, acc: 0.9765, valid_loss: 0.7129, valid_acc: 0.8202, \n",
      "Epoch 22, CIFAR-10 Batch 5:  loss: 0.1207, acc: 0.9703, valid_loss: 0.7156, valid_acc: 0.8162, \n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 0.1263, acc: 0.9641, valid_loss: 0.7471, valid_acc: 0.8222, \n",
      "Epoch 23, CIFAR-10 Batch 2:  loss: 0.1231, acc: 0.9604, valid_loss: 0.7263, valid_acc: 0.8164, \n",
      "Epoch 23, CIFAR-10 Batch 3:  loss: 0.1078, acc: 0.9703, valid_loss: 0.7247, valid_acc: 0.8230, \n",
      "Epoch 23, CIFAR-10 Batch 4:  loss: 0.0977, acc: 0.9777, valid_loss: 0.7680, valid_acc: 0.8192, \n",
      "Epoch 23, CIFAR-10 Batch 5:  loss: 0.1124, acc: 0.9653, valid_loss: 0.7522, valid_acc: 0.8080, \n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 0.1236, acc: 0.9790, valid_loss: 0.7498, valid_acc: 0.8238, \n",
      "Epoch 24, CIFAR-10 Batch 2:  loss: 0.1015, acc: 0.9703, valid_loss: 0.7737, valid_acc: 0.8200, \n",
      "Epoch 24, CIFAR-10 Batch 3:  loss: 0.0985, acc: 0.9728, valid_loss: 0.7446, valid_acc: 0.8244, \n",
      "Epoch 24, CIFAR-10 Batch 4:  loss: 0.0838, acc: 0.9802, valid_loss: 0.7447, valid_acc: 0.8170, \n",
      "Epoch 24, CIFAR-10 Batch 5:  loss: 0.0727, acc: 0.9765, valid_loss: 0.7194, valid_acc: 0.8168, \n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 0.0734, acc: 0.9777, valid_loss: 0.7522, valid_acc: 0.8192, \n",
      "Epoch 25, CIFAR-10 Batch 2:  loss: 0.0747, acc: 0.9777, valid_loss: 0.7688, valid_acc: 0.8290, \n",
      "Epoch 25, CIFAR-10 Batch 3:  loss: 0.0883, acc: 0.9728, valid_loss: 0.7606, valid_acc: 0.8174, \n",
      "Epoch 25, CIFAR-10 Batch 4:  loss: 0.0644, acc: 0.9802, valid_loss: 0.8087, valid_acc: 0.8206, \n",
      "Epoch 25, CIFAR-10 Batch 5:  loss: 0.0635, acc: 0.9876, valid_loss: 0.7752, valid_acc: 0.8138, \n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 0.0790, acc: 0.9802, valid_loss: 0.7408, valid_acc: 0.8278, \n",
      "Epoch 26, CIFAR-10 Batch 2:  loss: 0.0652, acc: 0.9864, valid_loss: 0.7620, valid_acc: 0.8244, \n",
      "Epoch 26, CIFAR-10 Batch 3:  loss: 0.0640, acc: 0.9839, valid_loss: 0.8122, valid_acc: 0.8092, \n",
      "Epoch 26, CIFAR-10 Batch 4:  loss: 0.0667, acc: 0.9839, valid_loss: 0.7762, valid_acc: 0.8240, \n",
      "Epoch 26, CIFAR-10 Batch 5:  loss: 0.0597, acc: 0.9839, valid_loss: 0.8081, valid_acc: 0.8128, \n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 0.0530, acc: 0.9876, valid_loss: 0.7658, valid_acc: 0.8280, \n",
      "Epoch 27, CIFAR-10 Batch 2:  loss: 0.0622, acc: 0.9827, valid_loss: 0.8174, valid_acc: 0.8208, \n",
      "Epoch 27, CIFAR-10 Batch 3:  loss: 0.0517, acc: 0.9839, valid_loss: 0.7903, valid_acc: 0.8268, \n",
      "Epoch 27, CIFAR-10 Batch 4:  loss: 0.0357, acc: 0.9926, valid_loss: 0.8141, valid_acc: 0.8252, \n",
      "Epoch 27, CIFAR-10 Batch 5:  loss: 0.0611, acc: 0.9827, valid_loss: 0.7810, valid_acc: 0.8174, \n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 0.0623, acc: 0.9814, valid_loss: 0.7932, valid_acc: 0.8272, \n",
      "Epoch 28, CIFAR-10 Batch 2:  loss: 0.0690, acc: 0.9839, valid_loss: 0.8751, valid_acc: 0.8132, \n",
      "Epoch 28, CIFAR-10 Batch 3:  loss: 0.0434, acc: 0.9864, valid_loss: 0.7646, valid_acc: 0.8258, \n",
      "Epoch 28, CIFAR-10 Batch 4:  loss: 0.0456, acc: 0.9864, valid_loss: 0.7838, valid_acc: 0.8280, \n",
      "Epoch 28, CIFAR-10 Batch 5:  loss: 0.0275, acc: 0.9975, valid_loss: 0.8115, valid_acc: 0.8260, \n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 0.0365, acc: 0.9913, valid_loss: 0.7980, valid_acc: 0.8310, \n",
      "Epoch 29, CIFAR-10 Batch 2:  loss: 0.0632, acc: 0.9814, valid_loss: 0.7686, valid_acc: 0.8298, \n",
      "Epoch 29, CIFAR-10 Batch 3:  loss: 0.0350, acc: 0.9938, valid_loss: 0.7628, valid_acc: 0.8312, \n",
      "Epoch 29, CIFAR-10 Batch 4:  loss: 0.0599, acc: 0.9864, valid_loss: 0.8272, valid_acc: 0.8216, \n",
      "Epoch 29, CIFAR-10 Batch 5:  loss: 0.0427, acc: 0.9889, valid_loss: 0.8620, valid_acc: 0.8244, \n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 0.0312, acc: 0.9938, valid_loss: 0.8101, valid_acc: 0.8338, \n",
      "Epoch 30, CIFAR-10 Batch 2:  loss: 0.0355, acc: 0.9938, valid_loss: 0.7457, valid_acc: 0.8400, \n",
      "Epoch 30, CIFAR-10 Batch 3:  loss: 0.0346, acc: 0.9913, valid_loss: 0.7954, valid_acc: 0.8262, \n",
      "Epoch 30, CIFAR-10 Batch 4:  loss: 0.0419, acc: 0.9926, valid_loss: 0.8726, valid_acc: 0.8280, \n",
      "Epoch 30, CIFAR-10 Batch 5:  loss: 0.0316, acc: 0.9950, valid_loss: 0.7893, valid_acc: 0.8286, \n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 0.0405, acc: 0.9913, valid_loss: 0.8010, valid_acc: 0.8294, \n",
      "Epoch 31, CIFAR-10 Batch 2:  loss: 0.0454, acc: 0.9864, valid_loss: 0.7887, valid_acc: 0.8328, \n",
      "Epoch 31, CIFAR-10 Batch 3:  loss: 0.0258, acc: 0.9963, valid_loss: 0.8168, valid_acc: 0.8282, \n",
      "Epoch 31, CIFAR-10 Batch 4:  loss: 0.0395, acc: 0.9938, valid_loss: 0.8515, valid_acc: 0.8236, \n",
      "Epoch 31, CIFAR-10 Batch 5:  loss: 0.0339, acc: 0.9876, valid_loss: 0.8215, valid_acc: 0.8290, \n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 0.0232, acc: 0.9963, valid_loss: 0.8590, valid_acc: 0.8314, \n",
      "Epoch 32, CIFAR-10 Batch 2:  loss: 0.0517, acc: 0.9864, valid_loss: 0.8031, valid_acc: 0.8276, \n",
      "Epoch 32, CIFAR-10 Batch 3:  loss: 0.0241, acc: 0.9926, valid_loss: 0.8244, valid_acc: 0.8340, \n",
      "Epoch 32, CIFAR-10 Batch 4:  loss: 0.0270, acc: 0.9950, valid_loss: 0.8692, valid_acc: 0.8290, \n",
      "Epoch 32, CIFAR-10 Batch 5:  loss: 0.0240, acc: 0.9938, valid_loss: 0.8181, valid_acc: 0.8362, \n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 0.0441, acc: 0.9938, valid_loss: 0.8318, valid_acc: 0.8288, \n",
      "Epoch 33, CIFAR-10 Batch 2:  loss: 0.0520, acc: 0.9864, valid_loss: 0.8006, valid_acc: 0.8318, \n",
      "Epoch 33, CIFAR-10 Batch 3:  loss: 0.0189, acc: 1.0000, valid_loss: 0.8061, valid_acc: 0.8342, \n",
      "Epoch 33, CIFAR-10 Batch 4:  loss: 0.0253, acc: 0.9975, valid_loss: 0.8241, valid_acc: 0.8336, \n",
      "Epoch 33, CIFAR-10 Batch 5:  loss: 0.0272, acc: 0.9963, valid_loss: 0.8664, valid_acc: 0.8248, \n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 0.0219, acc: 0.9963, valid_loss: 0.8537, valid_acc: 0.8314, \n",
      "Epoch 34, CIFAR-10 Batch 2:  loss: 0.0523, acc: 0.9889, valid_loss: 0.8453, valid_acc: 0.8282, \n",
      "Epoch 34, CIFAR-10 Batch 3:  loss: 0.0226, acc: 0.9963, valid_loss: 0.7994, valid_acc: 0.8330, \n",
      "Epoch 34, CIFAR-10 Batch 4:  loss: 0.0205, acc: 0.9975, valid_loss: 0.7819, valid_acc: 0.8436, \n",
      "Epoch 34, CIFAR-10 Batch 5:  loss: 0.0193, acc: 0.9963, valid_loss: 0.8591, valid_acc: 0.8318, \n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 0.0347, acc: 0.9913, valid_loss: 0.8048, valid_acc: 0.8366, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, CIFAR-10 Batch 2:  loss: 0.0298, acc: 0.9926, valid_loss: 0.7999, valid_acc: 0.8474, \n",
      "Epoch 35, CIFAR-10 Batch 3:  loss: 0.0155, acc: 0.9963, valid_loss: 0.8510, valid_acc: 0.8394, \n",
      "Epoch 35, CIFAR-10 Batch 4:  loss: 0.0262, acc: 0.9950, valid_loss: 0.9001, valid_acc: 0.8396, \n",
      "Epoch 35, CIFAR-10 Batch 5:  loss: 0.0092, acc: 1.0000, valid_loss: 0.9347, valid_acc: 0.8312, \n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 0.0169, acc: 0.9963, valid_loss: 0.8728, valid_acc: 0.8410, \n",
      "Epoch 36, CIFAR-10 Batch 2:  loss: 0.0174, acc: 0.9963, valid_loss: 0.8031, valid_acc: 0.8496, \n",
      "Epoch 36, CIFAR-10 Batch 3:  loss: 0.0087, acc: 0.9988, valid_loss: 0.8689, valid_acc: 0.8364, \n",
      "Epoch 36, CIFAR-10 Batch 4:  loss: 0.0184, acc: 0.9963, valid_loss: 0.8469, valid_acc: 0.8410, \n",
      "Epoch 36, CIFAR-10 Batch 5:  loss: 0.0133, acc: 0.9975, valid_loss: 0.9463, valid_acc: 0.8362, \n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 0.0217, acc: 0.9975, valid_loss: 0.8911, valid_acc: 0.8356, \n",
      "Epoch 37, CIFAR-10 Batch 2:  loss: 0.0268, acc: 0.9950, valid_loss: 0.8433, valid_acc: 0.8398, \n",
      "Epoch 37, CIFAR-10 Batch 3:  loss: 0.0074, acc: 0.9988, valid_loss: 0.8088, valid_acc: 0.8462, \n",
      "Epoch 37, CIFAR-10 Batch 4:  loss: 0.0185, acc: 0.9975, valid_loss: 0.8554, valid_acc: 0.8472, \n",
      "Epoch 37, CIFAR-10 Batch 5:  loss: 0.0131, acc: 0.9988, valid_loss: 0.8549, valid_acc: 0.8400, \n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 0.0152, acc: 0.9975, valid_loss: 0.9204, valid_acc: 0.8398, \n",
      "Epoch 38, CIFAR-10 Batch 2:  loss: 0.0331, acc: 0.9926, valid_loss: 0.8714, valid_acc: 0.8474, \n",
      "Epoch 38, CIFAR-10 Batch 3:  loss: 0.0126, acc: 0.9988, valid_loss: 0.8766, valid_acc: 0.8450, \n",
      "Epoch 38, CIFAR-10 Batch 4:  loss: 0.0109, acc: 0.9975, valid_loss: 0.9020, valid_acc: 0.8458, \n",
      "Epoch 38, CIFAR-10 Batch 5:  loss: 0.0080, acc: 1.0000, valid_loss: 0.8936, valid_acc: 0.8480, \n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 0.0131, acc: 0.9975, valid_loss: 0.8670, valid_acc: 0.8466, \n",
      "Epoch 39, CIFAR-10 Batch 2:  loss: 0.0194, acc: 0.9963, valid_loss: 0.8935, valid_acc: 0.8432, \n",
      "Epoch 39, CIFAR-10 Batch 3:  loss: 0.0062, acc: 1.0000, valid_loss: 0.8964, valid_acc: 0.8376, \n",
      "Epoch 39, CIFAR-10 Batch 4:  loss: 0.0240, acc: 0.9963, valid_loss: 0.9077, valid_acc: 0.8318, \n",
      "Epoch 39, CIFAR-10 Batch 5:  loss: 0.0142, acc: 0.9975, valid_loss: 0.8642, valid_acc: 0.8406, \n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 0.0253, acc: 0.9963, valid_loss: 0.8509, valid_acc: 0.8460, \n",
      "Epoch 40, CIFAR-10 Batch 2:  loss: 0.0131, acc: 0.9963, valid_loss: 0.8593, valid_acc: 0.8514, \n",
      "Epoch 40, CIFAR-10 Batch 3:  loss: 0.0040, acc: 1.0000, valid_loss: 0.9370, valid_acc: 0.8428, \n",
      "Epoch 40, CIFAR-10 Batch 4:  loss: 0.0083, acc: 0.9975, valid_loss: 0.9248, valid_acc: 0.8406, \n",
      "Epoch 40, CIFAR-10 Batch 5:  loss: 0.0050, acc: 1.0000, valid_loss: 0.9013, valid_acc: 0.8436, \n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 0.0183, acc: 0.9988, valid_loss: 0.8989, valid_acc: 0.8408, \n",
      "Epoch 41, CIFAR-10 Batch 2:  loss: 0.0128, acc: 0.9975, valid_loss: 0.8843, valid_acc: 0.8466, \n",
      "Epoch 41, CIFAR-10 Batch 3:  loss: 0.0065, acc: 1.0000, valid_loss: 0.8983, valid_acc: 0.8484, \n",
      "Epoch 41, CIFAR-10 Batch 4:  loss: 0.0076, acc: 0.9988, valid_loss: 0.8741, valid_acc: 0.8460, \n",
      "Epoch 41, CIFAR-10 Batch 5:  loss: 0.0054, acc: 1.0000, valid_loss: 0.8844, valid_acc: 0.8432, \n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 0.0075, acc: 0.9988, valid_loss: 0.9267, valid_acc: 0.8478, \n",
      "Epoch 42, CIFAR-10 Batch 2:  loss: 0.0095, acc: 0.9975, valid_loss: 0.9633, valid_acc: 0.8440, \n",
      "Epoch 42, CIFAR-10 Batch 3:  loss: 0.0124, acc: 0.9975, valid_loss: 0.9392, valid_acc: 0.8462, \n",
      "Epoch 42, CIFAR-10 Batch 4:  loss: 0.0087, acc: 0.9988, valid_loss: 0.8986, valid_acc: 0.8498, \n",
      "Epoch 42, CIFAR-10 Batch 5:  loss: 0.0060, acc: 1.0000, valid_loss: 0.8710, valid_acc: 0.8498, \n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 0.0070, acc: 0.9988, valid_loss: 0.9665, valid_acc: 0.8478, \n",
      "Epoch 43, CIFAR-10 Batch 2:  loss: 0.0104, acc: 0.9975, valid_loss: 0.9868, valid_acc: 0.8488, \n",
      "Epoch 43, CIFAR-10 Batch 3:  loss: 0.0050, acc: 1.0000, valid_loss: 0.9368, valid_acc: 0.8454, \n",
      "Epoch 43, CIFAR-10 Batch 4:  loss: 0.0027, acc: 1.0000, valid_loss: 0.9307, valid_acc: 0.8488, \n",
      "Epoch 43, CIFAR-10 Batch 5:  loss: 0.0032, acc: 1.0000, valid_loss: 0.9009, valid_acc: 0.8428, \n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 0.0141, acc: 0.9963, valid_loss: 0.9035, valid_acc: 0.8458, \n",
      "Epoch 44, CIFAR-10 Batch 2:  loss: 0.0085, acc: 0.9988, valid_loss: 0.9182, valid_acc: 0.8452, \n",
      "Epoch 44, CIFAR-10 Batch 3:  loss: 0.0044, acc: 1.0000, valid_loss: 0.9383, valid_acc: 0.8436, \n",
      "Epoch 44, CIFAR-10 Batch 4:  loss: 0.0042, acc: 0.9988, valid_loss: 0.9310, valid_acc: 0.8460, \n",
      "Epoch 44, CIFAR-10 Batch 5:  loss: 0.0032, acc: 1.0000, valid_loss: 0.9540, valid_acc: 0.8460, \n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 0.0165, acc: 0.9988, valid_loss: 0.9726, valid_acc: 0.8470, \n",
      "Epoch 45, CIFAR-10 Batch 2:  loss: 0.0134, acc: 0.9963, valid_loss: 0.9495, valid_acc: 0.8498, \n",
      "Epoch 45, CIFAR-10 Batch 3:  loss: 0.0041, acc: 1.0000, valid_loss: 0.9788, valid_acc: 0.8428, \n",
      "Epoch 45, CIFAR-10 Batch 4:  loss: 0.0043, acc: 0.9988, valid_loss: 0.8711, valid_acc: 0.8508, \n",
      "Epoch 45, CIFAR-10 Batch 5:  loss: 0.0036, acc: 1.0000, valid_loss: 0.9194, valid_acc: 0.8466, \n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 0.0066, acc: 0.9988, valid_loss: 0.9239, valid_acc: 0.8444, \n",
      "Epoch 46, CIFAR-10 Batch 2:  loss: 0.0076, acc: 0.9988, valid_loss: 0.9362, valid_acc: 0.8452, \n",
      "Epoch 46, CIFAR-10 Batch 3:  loss: 0.0081, acc: 0.9988, valid_loss: 0.9544, valid_acc: 0.8444, \n",
      "Epoch 46, CIFAR-10 Batch 4:  loss: 0.0044, acc: 1.0000, valid_loss: 0.9232, valid_acc: 0.8444, \n",
      "Epoch 46, CIFAR-10 Batch 5:  loss: 0.0066, acc: 0.9988, valid_loss: 0.9099, valid_acc: 0.8478, \n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 0.0102, acc: 0.9975, valid_loss: 0.8959, valid_acc: 0.8478, \n",
      "Epoch 47, CIFAR-10 Batch 2:  loss: 0.0101, acc: 0.9963, valid_loss: 0.9240, valid_acc: 0.8424, \n",
      "Epoch 47, CIFAR-10 Batch 3:  loss: 0.0028, acc: 1.0000, valid_loss: 0.9563, valid_acc: 0.8430, \n",
      "Epoch 47, CIFAR-10 Batch 4:  loss: 0.0061, acc: 0.9988, valid_loss: 0.9618, valid_acc: 0.8438, \n",
      "Epoch 47, CIFAR-10 Batch 5:  loss: 0.0027, acc: 1.0000, valid_loss: 0.9233, valid_acc: 0.8484, \n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 0.0110, acc: 0.9988, valid_loss: 0.9062, valid_acc: 0.8536, \n",
      "Epoch 48, CIFAR-10 Batch 2:  loss: 0.0042, acc: 0.9988, valid_loss: 0.9124, valid_acc: 0.8532, \n",
      "Epoch 48, CIFAR-10 Batch 3:  loss: 0.0024, acc: 1.0000, valid_loss: 0.9859, valid_acc: 0.8432, \n",
      "Epoch 48, CIFAR-10 Batch 4:  loss: 0.0077, acc: 0.9988, valid_loss: 0.9480, valid_acc: 0.8478, \n",
      "Epoch 48, CIFAR-10 Batch 5:  loss: 0.0035, acc: 0.9988, valid_loss: 0.9429, valid_acc: 0.8442, \n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 0.0079, acc: 0.9988, valid_loss: 0.9094, valid_acc: 0.8500, \n",
      "Epoch 49, CIFAR-10 Batch 2:  loss: 0.0149, acc: 0.9975, valid_loss: 0.9251, valid_acc: 0.8458, \n",
      "Epoch 49, CIFAR-10 Batch 3:  loss: 0.0070, acc: 0.9988, valid_loss: 0.9083, valid_acc: 0.8414, \n",
      "Epoch 49, CIFAR-10 Batch 4:  loss: 0.0079, acc: 0.9975, valid_loss: 0.9236, valid_acc: 0.8430, \n",
      "Epoch 49, CIFAR-10 Batch 5:  loss: 0.0052, acc: 1.0000, valid_loss: 0.8901, valid_acc: 0.8492, \n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 0.0196, acc: 0.9975, valid_loss: 0.9218, valid_acc: 0.8452, \n",
      "Epoch 50, CIFAR-10 Batch 2:  loss: 0.0054, acc: 0.9988, valid_loss: 0.9169, valid_acc: 0.8500, \n",
      "Epoch 50, CIFAR-10 Batch 3:  loss: 0.0045, acc: 1.0000, valid_loss: 0.9126, valid_acc: 0.8510, \n",
      "Epoch 50, CIFAR-10 Batch 4:  loss: 0.0026, acc: 1.0000, valid_loss: 0.8825, valid_acc: 0.8544, \n",
      "Epoch 50, CIFAR-10 Batch 5:  loss: 0.0035, acc: 1.0000, valid_loss: 0.9632, valid_acc: 0.8418, \n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 0.0149, acc: 0.9988, valid_loss: 0.9403, valid_acc: 0.8466, \n",
      "Epoch 51, CIFAR-10 Batch 2:  loss: 0.0099, acc: 0.9988, valid_loss: 0.8642, valid_acc: 0.8496, \n",
      "Epoch 51, CIFAR-10 Batch 3:  loss: 0.0052, acc: 0.9988, valid_loss: 0.8752, valid_acc: 0.8466, \n",
      "Epoch 51, CIFAR-10 Batch 4:  loss: 0.0055, acc: 0.9988, valid_loss: 0.9489, valid_acc: 0.8464, \n",
      "Epoch 51, CIFAR-10 Batch 5:  loss: 0.0074, acc: 0.9975, valid_loss: 0.8903, valid_acc: 0.8472, \n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 0.0095, acc: 0.9963, valid_loss: 0.9200, valid_acc: 0.8432, \n",
      "Epoch 52, CIFAR-10 Batch 2:  loss: 0.0125, acc: 0.9988, valid_loss: 0.8606, valid_acc: 0.8546, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, CIFAR-10 Batch 3:  loss: 0.0043, acc: 0.9988, valid_loss: 0.9509, valid_acc: 0.8494, \n",
      "Epoch 52, CIFAR-10 Batch 4:  loss: 0.0042, acc: 1.0000, valid_loss: 0.9128, valid_acc: 0.8474, \n",
      "Epoch 52, CIFAR-10 Batch 5:  loss: 0.0094, acc: 0.9975, valid_loss: 0.9085, valid_acc: 0.8410, \n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 0.0152, acc: 0.9975, valid_loss: 0.8892, valid_acc: 0.8510, \n",
      "Epoch 53, CIFAR-10 Batch 2:  loss: 0.0099, acc: 0.9988, valid_loss: 0.9163, valid_acc: 0.8486, \n",
      "Epoch 53, CIFAR-10 Batch 3:  loss: 0.0041, acc: 1.0000, valid_loss: 0.9391, valid_acc: 0.8458, \n",
      "Epoch 53, CIFAR-10 Batch 4:  loss: 0.0062, acc: 1.0000, valid_loss: 0.8961, valid_acc: 0.8472, \n",
      "Epoch 53, CIFAR-10 Batch 5:  loss: 0.0072, acc: 0.9988, valid_loss: 0.8746, valid_acc: 0.8506, \n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 0.0144, acc: 0.9988, valid_loss: 0.9057, valid_acc: 0.8478, \n",
      "Epoch 54, CIFAR-10 Batch 2:  loss: 0.0309, acc: 0.9950, valid_loss: 0.8963, valid_acc: 0.8458, \n",
      "Epoch 54, CIFAR-10 Batch 3:  loss: 0.0061, acc: 0.9988, valid_loss: 0.9002, valid_acc: 0.8480, \n",
      "Epoch 54, CIFAR-10 Batch 4:  loss: 0.0028, acc: 1.0000, valid_loss: 0.9562, valid_acc: 0.8476, \n",
      "Epoch 54, CIFAR-10 Batch 5:  loss: 0.0023, acc: 1.0000, valid_loss: 0.9450, valid_acc: 0.8538, \n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 0.0030, acc: 1.0000, valid_loss: 0.9972, valid_acc: 0.8464, \n",
      "Epoch 55, CIFAR-10 Batch 2:  loss: 0.0120, acc: 0.9988, valid_loss: 0.9690, valid_acc: 0.8460, \n",
      "Epoch 55, CIFAR-10 Batch 3:  loss: 0.0024, acc: 1.0000, valid_loss: 0.9480, valid_acc: 0.8406, \n",
      "Epoch 55, CIFAR-10 Batch 4:  loss: 0.0028, acc: 1.0000, valid_loss: 0.9518, valid_acc: 0.8500, \n",
      "Epoch 55, CIFAR-10 Batch 5:  loss: 0.0018, acc: 1.0000, valid_loss: 0.9506, valid_acc: 0.8526, \n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 0.0024, acc: 1.0000, valid_loss: 0.9857, valid_acc: 0.8488, \n",
      "Epoch 56, CIFAR-10 Batch 2:  loss: 0.0078, acc: 0.9988, valid_loss: 0.9754, valid_acc: 0.8516, \n",
      "Epoch 56, CIFAR-10 Batch 3:  loss: 0.0019, acc: 1.0000, valid_loss: 1.0373, valid_acc: 0.8480, \n",
      "Epoch 56, CIFAR-10 Batch 4:  loss: 0.0018, acc: 1.0000, valid_loss: 1.0985, valid_acc: 0.8452, \n",
      "Epoch 56, CIFAR-10 Batch 5:  loss: 0.0031, acc: 1.0000, valid_loss: 1.0129, valid_acc: 0.8448, \n",
      "Epoch 57, CIFAR-10 Batch 1:  loss: 0.0026, acc: 1.0000, valid_loss: 0.9198, valid_acc: 0.8508, \n",
      "Epoch 57, CIFAR-10 Batch 2:  loss: 0.0040, acc: 0.9988, valid_loss: 0.9489, valid_acc: 0.8512, \n",
      "Epoch 57, CIFAR-10 Batch 3:  loss: 0.0047, acc: 0.9988, valid_loss: 0.9840, valid_acc: 0.8462, \n",
      "Epoch 57, CIFAR-10 Batch 4:  loss: 0.0038, acc: 1.0000, valid_loss: 1.0303, valid_acc: 0.8442, \n",
      "Epoch 57, CIFAR-10 Batch 5:  loss: 0.0034, acc: 1.0000, valid_loss: 1.0155, valid_acc: 0.8424, \n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 0.0024, acc: 1.0000, valid_loss: 1.0262, valid_acc: 0.8478, \n",
      "Epoch 58, CIFAR-10 Batch 2:  loss: 0.0049, acc: 0.9988, valid_loss: 0.9828, valid_acc: 0.8422, \n",
      "Epoch 58, CIFAR-10 Batch 3:  loss: 0.0054, acc: 1.0000, valid_loss: 0.9536, valid_acc: 0.8430, \n",
      "Epoch 58, CIFAR-10 Batch 4:  loss: 0.0050, acc: 1.0000, valid_loss: 0.9031, valid_acc: 0.8476, \n",
      "Epoch 58, CIFAR-10 Batch 5:  loss: 0.0061, acc: 0.9988, valid_loss: 0.9555, valid_acc: 0.8434, \n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 0.0032, acc: 1.0000, valid_loss: 0.8917, valid_acc: 0.8460, \n",
      "Epoch 59, CIFAR-10 Batch 2:  loss: 0.0083, acc: 0.9988, valid_loss: 0.9991, valid_acc: 0.8442, \n",
      "Epoch 59, CIFAR-10 Batch 3:  loss: 0.0051, acc: 0.9988, valid_loss: 1.0339, valid_acc: 0.8440, \n",
      "Epoch 59, CIFAR-10 Batch 4:  loss: 0.0038, acc: 1.0000, valid_loss: 1.0705, valid_acc: 0.8366, \n",
      "Epoch 59, CIFAR-10 Batch 5:  loss: 0.0045, acc: 0.9988, valid_loss: 0.9792, valid_acc: 0.8416, \n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 0.0085, acc: 0.9988, valid_loss: 0.9091, valid_acc: 0.8442, \n",
      "Epoch 60, CIFAR-10 Batch 2:  loss: 0.0070, acc: 1.0000, valid_loss: 0.8887, valid_acc: 0.8450, \n",
      "Epoch 60, CIFAR-10 Batch 3:  loss: 0.0074, acc: 0.9988, valid_loss: 0.9532, valid_acc: 0.8448, \n",
      "Epoch 60, CIFAR-10 Batch 4:  loss: 0.0013, acc: 1.0000, valid_loss: 1.0743, valid_acc: 0.8478, \n",
      "Epoch 60, CIFAR-10 Batch 5:  loss: 0.0020, acc: 1.0000, valid_loss: 1.0137, valid_acc: 0.8444, \n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 0.0086, acc: 0.9988, valid_loss: 0.9714, valid_acc: 0.8392, \n",
      "Epoch 61, CIFAR-10 Batch 2:  loss: 0.0078, acc: 0.9988, valid_loss: 0.9573, valid_acc: 0.8420, \n",
      "Epoch 61, CIFAR-10 Batch 3:  loss: 0.0059, acc: 0.9988, valid_loss: 0.9344, valid_acc: 0.8402, \n",
      "Epoch 61, CIFAR-10 Batch 4:  loss: 0.0047, acc: 0.9988, valid_loss: 0.9326, valid_acc: 0.8452, \n",
      "Epoch 61, CIFAR-10 Batch 5:  loss: 0.0016, acc: 1.0000, valid_loss: 0.9686, valid_acc: 0.8470, \n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 0.0018, acc: 1.0000, valid_loss: 1.0884, valid_acc: 0.8408, \n",
      "Epoch 62, CIFAR-10 Batch 2:  loss: 0.0028, acc: 1.0000, valid_loss: 1.0343, valid_acc: 0.8500, \n",
      "Epoch 62, CIFAR-10 Batch 3:  loss: 0.0016, acc: 1.0000, valid_loss: 0.9689, valid_acc: 0.8542, \n",
      "Epoch 62, CIFAR-10 Batch 4:  loss: 0.0040, acc: 0.9988, valid_loss: 0.9748, valid_acc: 0.8512, \n",
      "Epoch 62, CIFAR-10 Batch 5:  loss: 0.0016, acc: 1.0000, valid_loss: 1.0177, valid_acc: 0.8482, \n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 0.0023, acc: 1.0000, valid_loss: 1.0618, valid_acc: 0.8464, \n",
      "Epoch 63, CIFAR-10 Batch 2:  loss: 0.0142, acc: 0.9963, valid_loss: 1.0408, valid_acc: 0.8410, \n",
      "Epoch 63, CIFAR-10 Batch 3:  loss: 0.0034, acc: 0.9988, valid_loss: 0.9888, valid_acc: 0.8386, \n",
      "Epoch 63, CIFAR-10 Batch 4:  loss: 0.0135, acc: 0.9975, valid_loss: 0.9897, valid_acc: 0.8394, \n",
      "Epoch 63, CIFAR-10 Batch 5:  loss: 0.0062, acc: 0.9988, valid_loss: 0.8876, valid_acc: 0.8460, \n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 0.0035, acc: 1.0000, valid_loss: 0.9264, valid_acc: 0.8476, \n",
      "Epoch 64, CIFAR-10 Batch 2:  loss: 0.0055, acc: 0.9988, valid_loss: 1.0196, valid_acc: 0.8434, \n",
      "Epoch 64, CIFAR-10 Batch 3:  loss: 0.0016, acc: 1.0000, valid_loss: 1.0385, valid_acc: 0.8456, \n",
      "Epoch 64, CIFAR-10 Batch 4:  loss: 0.0018, acc: 1.0000, valid_loss: 1.0360, valid_acc: 0.8464, \n",
      "Epoch 64, CIFAR-10 Batch 5:  loss: 0.0034, acc: 1.0000, valid_loss: 1.0197, valid_acc: 0.8468, \n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 0.0052, acc: 1.0000, valid_loss: 0.9152, valid_acc: 0.8458, \n",
      "Epoch 65, CIFAR-10 Batch 2:  loss: 0.0074, acc: 0.9988, valid_loss: 0.8962, valid_acc: 0.8500, \n",
      "Epoch 65, CIFAR-10 Batch 3:  loss: 0.0017, acc: 1.0000, valid_loss: 0.9783, valid_acc: 0.8552, \n",
      "Epoch 65, CIFAR-10 Batch 4:  loss: 0.0016, acc: 1.0000, valid_loss: 1.1138, valid_acc: 0.8404, \n",
      "Epoch 65, CIFAR-10 Batch 5:  loss: 0.0035, acc: 1.0000, valid_loss: 1.0417, valid_acc: 0.8458, \n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 0.0024, acc: 1.0000, valid_loss: 1.0407, valid_acc: 0.8398, \n",
      "Epoch 66, CIFAR-10 Batch 2:  loss: 0.0033, acc: 0.9988, valid_loss: 1.0113, valid_acc: 0.8476, \n",
      "Epoch 66, CIFAR-10 Batch 3:  loss: 0.0020, acc: 1.0000, valid_loss: 0.9515, valid_acc: 0.8540, \n",
      "Epoch 66, CIFAR-10 Batch 4:  loss: 0.0023, acc: 1.0000, valid_loss: 1.0119, valid_acc: 0.8494, \n",
      "Epoch 66, CIFAR-10 Batch 5:  loss: 0.0018, acc: 1.0000, valid_loss: 1.0396, valid_acc: 0.8468, \n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 0.0034, acc: 1.0000, valid_loss: 0.9980, valid_acc: 0.8450, \n",
      "Epoch 67, CIFAR-10 Batch 2:  loss: 0.0103, acc: 0.9988, valid_loss: 1.0330, valid_acc: 0.8432, \n",
      "Epoch 67, CIFAR-10 Batch 3:  loss: 0.0048, acc: 0.9988, valid_loss: 0.9897, valid_acc: 0.8428, \n",
      "Epoch 67, CIFAR-10 Batch 4:  loss: 0.0029, acc: 1.0000, valid_loss: 0.9690, valid_acc: 0.8506, \n",
      "Epoch 67, CIFAR-10 Batch 5:  loss: 0.0059, acc: 0.9988, valid_loss: 0.9843, valid_acc: 0.8442, \n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 0.0029, acc: 1.0000, valid_loss: 0.9124, valid_acc: 0.8510, \n",
      "Epoch 68, CIFAR-10 Batch 2:  loss: 0.0049, acc: 0.9988, valid_loss: 1.0046, valid_acc: 0.8466, \n",
      "Epoch 68, CIFAR-10 Batch 3:  loss: 0.0014, acc: 1.0000, valid_loss: 0.9441, valid_acc: 0.8530, \n",
      "Epoch 68, CIFAR-10 Batch 4:  loss: 0.0023, acc: 1.0000, valid_loss: 1.0280, valid_acc: 0.8470, \n",
      "Epoch 68, CIFAR-10 Batch 5:  loss: 0.0021, acc: 1.0000, valid_loss: 1.0959, valid_acc: 0.8426, \n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 0.0021, acc: 1.0000, valid_loss: 1.0569, valid_acc: 0.8454, \n",
      "Epoch 69, CIFAR-10 Batch 2:  loss: 0.0029, acc: 1.0000, valid_loss: 1.0152, valid_acc: 0.8522, \n",
      "Epoch 69, CIFAR-10 Batch 3:  loss: 0.0022, acc: 1.0000, valid_loss: 0.9885, valid_acc: 0.8498, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69, CIFAR-10 Batch 4:  loss: 0.0021, acc: 1.0000, valid_loss: 0.9423, valid_acc: 0.8492, \n",
      "Epoch 69, CIFAR-10 Batch 5:  loss: 0.0020, acc: 1.0000, valid_loss: 0.9374, valid_acc: 0.8514, \n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 0.0023, acc: 1.0000, valid_loss: 0.9805, valid_acc: 0.8524, \n",
      "Epoch 70, CIFAR-10 Batch 2:  loss: 0.0034, acc: 0.9988, valid_loss: 1.0312, valid_acc: 0.8516, \n",
      "Epoch 70, CIFAR-10 Batch 3:  loss: 0.0023, acc: 1.0000, valid_loss: 1.0190, valid_acc: 0.8540, \n",
      "Epoch 70, CIFAR-10 Batch 4:  loss: 0.0120, acc: 0.9950, valid_loss: 0.9759, valid_acc: 0.8430, \n",
      "Epoch 70, CIFAR-10 Batch 5:  loss: 0.0040, acc: 0.9988, valid_loss: 0.9188, valid_acc: 0.8466, \n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 0.0024, acc: 1.0000, valid_loss: 0.9434, valid_acc: 0.8494, \n",
      "Epoch 71, CIFAR-10 Batch 2:  loss: 0.0034, acc: 0.9988, valid_loss: 1.0150, valid_acc: 0.8560, \n",
      "Epoch 71, CIFAR-10 Batch 3:  loss: 0.0010, acc: 1.0000, valid_loss: 1.0217, valid_acc: 0.8556, \n",
      "Epoch 71, CIFAR-10 Batch 4:  loss: 0.0011, acc: 1.0000, valid_loss: 1.0096, valid_acc: 0.8534, \n",
      "Epoch 71, CIFAR-10 Batch 5:  loss: 0.0013, acc: 1.0000, valid_loss: 0.9999, valid_acc: 0.8478, \n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 0.0021, acc: 1.0000, valid_loss: 0.9329, valid_acc: 0.8478, \n",
      "Epoch 72, CIFAR-10 Batch 2:  loss: 0.0056, acc: 0.9988, valid_loss: 0.9537, valid_acc: 0.8486, \n",
      "Epoch 72, CIFAR-10 Batch 3:  loss: 0.0065, acc: 0.9988, valid_loss: 0.9949, valid_acc: 0.8450, \n",
      "Epoch 72, CIFAR-10 Batch 4:  loss: 0.0035, acc: 1.0000, valid_loss: 0.9656, valid_acc: 0.8512, \n",
      "Epoch 72, CIFAR-10 Batch 5:  loss: 0.0019, acc: 1.0000, valid_loss: 1.0149, valid_acc: 0.8472, \n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 0.0015, acc: 1.0000, valid_loss: 1.0510, valid_acc: 0.8538, \n",
      "Epoch 73, CIFAR-10 Batch 2:  loss: 0.0074, acc: 0.9988, valid_loss: 0.9884, valid_acc: 0.8498, \n",
      "Epoch 73, CIFAR-10 Batch 3:  loss: 0.0019, acc: 1.0000, valid_loss: 0.9253, valid_acc: 0.8466, \n",
      "Epoch 73, CIFAR-10 Batch 4:  loss: 0.0027, acc: 1.0000, valid_loss: 1.0006, valid_acc: 0.8552, \n",
      "Epoch 73, CIFAR-10 Batch 5:  loss: 0.0017, acc: 1.0000, valid_loss: 1.0166, valid_acc: 0.8498, \n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 0.0007, acc: 1.0000, valid_loss: 1.0625, valid_acc: 0.8502, \n",
      "Epoch 74, CIFAR-10 Batch 2:  loss: 0.0027, acc: 1.0000, valid_loss: 1.0415, valid_acc: 0.8532, \n",
      "Epoch 74, CIFAR-10 Batch 3:  loss: 0.0014, acc: 1.0000, valid_loss: 1.0124, valid_acc: 0.8464, \n",
      "Epoch 74, CIFAR-10 Batch 4:  loss: 0.0028, acc: 1.0000, valid_loss: 0.9663, valid_acc: 0.8470, \n",
      "Epoch 74, CIFAR-10 Batch 5:  loss: 0.0031, acc: 0.9988, valid_loss: 0.9531, valid_acc: 0.8532, \n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 0.0034, acc: 1.0000, valid_loss: 0.9727, valid_acc: 0.8516, \n",
      "Epoch 75, CIFAR-10 Batch 2:  loss: 0.0037, acc: 0.9988, valid_loss: 0.9787, valid_acc: 0.8550, \n",
      "Epoch 75, CIFAR-10 Batch 3:  loss: 0.0013, acc: 1.0000, valid_loss: 1.0246, valid_acc: 0.8474, \n",
      "Epoch 75, CIFAR-10 Batch 4:  loss: 0.0024, acc: 1.0000, valid_loss: 1.0796, valid_acc: 0.8464, \n",
      "Epoch 75, CIFAR-10 Batch 5:  loss: 0.0051, acc: 0.9988, valid_loss: 1.0854, valid_acc: 0.8488, \n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 0.0042, acc: 1.0000, valid_loss: 1.0429, valid_acc: 0.8508, \n",
      "Epoch 76, CIFAR-10 Batch 2:  loss: 0.0033, acc: 1.0000, valid_loss: 0.9937, valid_acc: 0.8496, \n",
      "Epoch 76, CIFAR-10 Batch 3:  loss: 0.0049, acc: 0.9988, valid_loss: 1.0042, valid_acc: 0.8446, \n",
      "Epoch 76, CIFAR-10 Batch 4:  loss: 0.0018, acc: 1.0000, valid_loss: 1.0374, valid_acc: 0.8494, \n",
      "Epoch 76, CIFAR-10 Batch 5:  loss: 0.0034, acc: 1.0000, valid_loss: 1.0848, valid_acc: 0.8476, \n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 0.0016, acc: 1.0000, valid_loss: 1.0357, valid_acc: 0.8490, \n",
      "Epoch 77, CIFAR-10 Batch 2:  loss: 0.0066, acc: 0.9988, valid_loss: 1.0953, valid_acc: 0.8470, \n",
      "Epoch 77, CIFAR-10 Batch 3:  loss: 0.0029, acc: 1.0000, valid_loss: 1.0422, valid_acc: 0.8456, \n",
      "Epoch 77, CIFAR-10 Batch 4:  loss: 0.0015, acc: 1.0000, valid_loss: 0.9595, valid_acc: 0.8470, \n",
      "Epoch 77, CIFAR-10 Batch 5:  loss: 0.0022, acc: 1.0000, valid_loss: 0.9715, valid_acc: 0.8518, \n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 0.0036, acc: 0.9988, valid_loss: 0.9979, valid_acc: 0.8488, \n",
      "Epoch 78, CIFAR-10 Batch 2:  loss: 0.0016, acc: 1.0000, valid_loss: 1.0726, valid_acc: 0.8516, \n",
      "Epoch 78, CIFAR-10 Batch 3:  loss: 0.0012, acc: 1.0000, valid_loss: 1.0557, valid_acc: 0.8486, \n",
      "Epoch 78, CIFAR-10 Batch 4:  loss: 0.0025, acc: 1.0000, valid_loss: 0.9839, valid_acc: 0.8504, \n",
      "Epoch 78, CIFAR-10 Batch 5:  loss: 0.0017, acc: 1.0000, valid_loss: 0.9617, valid_acc: 0.8474, \n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 0.0035, acc: 0.9988, valid_loss: 0.9742, valid_acc: 0.8532, \n",
      "Epoch 79, CIFAR-10 Batch 2:  loss: 0.0029, acc: 1.0000, valid_loss: 1.0120, valid_acc: 0.8440, \n",
      "Epoch 79, CIFAR-10 Batch 3:  loss: 0.0021, acc: 1.0000, valid_loss: 1.0364, valid_acc: 0.8478, \n",
      "Epoch 79, CIFAR-10 Batch 4:  loss: 0.0061, acc: 0.9988, valid_loss: 1.0856, valid_acc: 0.8438, \n",
      "Epoch 79, CIFAR-10 Batch 5:  loss: 0.0074, acc: 0.9988, valid_loss: 0.9824, valid_acc: 0.8504, \n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 0.0028, acc: 1.0000, valid_loss: 1.0659, valid_acc: 0.8506, \n",
      "Epoch 80, CIFAR-10 Batch 2:  loss: 0.0021, acc: 1.0000, valid_loss: 1.0526, valid_acc: 0.8486, \n",
      "Epoch 80, CIFAR-10 Batch 3:  loss: 0.0021, acc: 1.0000, valid_loss: 1.0660, valid_acc: 0.8488, \n",
      "Epoch 80, CIFAR-10 Batch 4:  loss: 0.0007, acc: 1.0000, valid_loss: 1.1091, valid_acc: 0.8468, \n",
      "Epoch 80, CIFAR-10 Batch 5:  loss: 0.0013, acc: 1.0000, valid_loss: 1.1353, valid_acc: 0.8442, \n",
      "Epoch 81, CIFAR-10 Batch 1:  loss: 0.0028, acc: 1.0000, valid_loss: 1.0446, valid_acc: 0.8456, \n",
      "Epoch 81, CIFAR-10 Batch 2:  loss: 0.0089, acc: 0.9988, valid_loss: 0.9889, valid_acc: 0.8474, \n",
      "Epoch 81, CIFAR-10 Batch 3:  loss: 0.0025, acc: 1.0000, valid_loss: 0.9963, valid_acc: 0.8528, \n",
      "Epoch 81, CIFAR-10 Batch 4:  loss: 0.0011, acc: 1.0000, valid_loss: 1.0471, valid_acc: 0.8560, \n",
      "Epoch 81, CIFAR-10 Batch 5:  loss: 0.0011, acc: 1.0000, valid_loss: 1.0577, valid_acc: 0.8564, \n",
      "Epoch 82, CIFAR-10 Batch 1:  loss: 0.0008, acc: 1.0000, valid_loss: 1.0867, valid_acc: 0.8528, \n",
      "Epoch 82, CIFAR-10 Batch 2:  loss: 0.0011, acc: 1.0000, valid_loss: 1.1235, valid_acc: 0.8496, \n",
      "Epoch 82, CIFAR-10 Batch 3:  loss: 0.0007, acc: 1.0000, valid_loss: 1.1373, valid_acc: 0.8516, \n",
      "Epoch 82, CIFAR-10 Batch 4:  loss: 0.0010, acc: 1.0000, valid_loss: 1.0641, valid_acc: 0.8494, \n",
      "Epoch 82, CIFAR-10 Batch 5:  loss: 0.0021, acc: 1.0000, valid_loss: 1.1568, valid_acc: 0.8466, \n",
      "Epoch 83, CIFAR-10 Batch 1:  loss: 0.0015, acc: 1.0000, valid_loss: 1.1508, valid_acc: 0.8480, \n",
      "Epoch 83, CIFAR-10 Batch 2:  loss: 0.0045, acc: 1.0000, valid_loss: 1.0938, valid_acc: 0.8452, \n",
      "Epoch 83, CIFAR-10 Batch 3:  loss: 0.0086, acc: 0.9975, valid_loss: 0.9888, valid_acc: 0.8480, \n",
      "Epoch 83, CIFAR-10 Batch 4:  loss: 0.0047, acc: 1.0000, valid_loss: 0.9693, valid_acc: 0.8444, \n",
      "Epoch 83, CIFAR-10 Batch 5:  loss: 0.0015, acc: 1.0000, valid_loss: 0.9781, valid_acc: 0.8548, \n",
      "Epoch 84, CIFAR-10 Batch 1:  loss: 0.0022, acc: 1.0000, valid_loss: 1.0949, valid_acc: 0.8522, \n",
      "Epoch 84, CIFAR-10 Batch 2:  loss: 0.0015, acc: 1.0000, valid_loss: 1.1006, valid_acc: 0.8548, \n",
      "Epoch 84, CIFAR-10 Batch 3:  loss: 0.0014, acc: 1.0000, valid_loss: 1.0248, valid_acc: 0.8538, \n",
      "Epoch 84, CIFAR-10 Batch 4:  loss: 0.0018, acc: 1.0000, valid_loss: 1.0758, valid_acc: 0.8504, \n",
      "Epoch 84, CIFAR-10 Batch 5:  loss: 0.0014, acc: 1.0000, valid_loss: 1.0809, valid_acc: 0.8510, \n",
      "Epoch 85, CIFAR-10 Batch 1:  loss: 0.0025, acc: 1.0000, valid_loss: 1.0585, valid_acc: 0.8506, \n",
      "Epoch 85, CIFAR-10 Batch 2:  loss: 0.0013, acc: 1.0000, valid_loss: 1.0620, valid_acc: 0.8514, \n",
      "Epoch 85, CIFAR-10 Batch 3:  loss: 0.0019, acc: 1.0000, valid_loss: 1.0550, valid_acc: 0.8490, \n",
      "Epoch 85, CIFAR-10 Batch 4:  loss: 0.0007, acc: 1.0000, valid_loss: 1.0618, valid_acc: 0.8544, \n",
      "Epoch 85, CIFAR-10 Batch 5:  loss: 0.0010, acc: 1.0000, valid_loss: 1.1316, valid_acc: 0.8592, \n",
      "Epoch 86, CIFAR-10 Batch 1:  loss: 0.0011, acc: 1.0000, valid_loss: 1.1001, valid_acc: 0.8494, \n",
      "Epoch 86, CIFAR-10 Batch 2:  loss: 0.0040, acc: 1.0000, valid_loss: 1.0102, valid_acc: 0.8504, \n",
      "Epoch 86, CIFAR-10 Batch 3:  loss: 0.0036, acc: 0.9988, valid_loss: 1.0099, valid_acc: 0.8454, \n",
      "Epoch 86, CIFAR-10 Batch 4:  loss: 0.0038, acc: 0.9988, valid_loss: 1.0885, valid_acc: 0.8486, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, CIFAR-10 Batch 5:  loss: 0.0049, acc: 0.9988, valid_loss: 1.1426, valid_acc: 0.8466, \n",
      "Epoch 87, CIFAR-10 Batch 1:  loss: 0.0081, acc: 0.9988, valid_loss: 0.9505, valid_acc: 0.8420, \n",
      "Epoch 87, CIFAR-10 Batch 2:  loss: 0.0082, acc: 0.9975, valid_loss: 0.8963, valid_acc: 0.8516, \n",
      "Epoch 87, CIFAR-10 Batch 3:  loss: 0.0025, acc: 1.0000, valid_loss: 1.0302, valid_acc: 0.8522, \n",
      "Epoch 87, CIFAR-10 Batch 4:  loss: 0.0038, acc: 1.0000, valid_loss: 1.0646, valid_acc: 0.8504, \n",
      "Epoch 87, CIFAR-10 Batch 5:  loss: 0.0053, acc: 0.9988, valid_loss: 0.9736, valid_acc: 0.8488, \n",
      "Epoch 88, CIFAR-10 Batch 1:  loss: 0.0038, acc: 0.9988, valid_loss: 0.9618, valid_acc: 0.8512, \n",
      "Epoch 88, CIFAR-10 Batch 2:  loss: 0.0033, acc: 0.9988, valid_loss: 1.0532, valid_acc: 0.8540, \n",
      "Epoch 88, CIFAR-10 Batch 3:  loss: 0.0012, acc: 1.0000, valid_loss: 1.0929, valid_acc: 0.8548, \n",
      "Epoch 88, CIFAR-10 Batch 4:  loss: 0.0033, acc: 1.0000, valid_loss: 1.0275, valid_acc: 0.8430, \n",
      "Epoch 88, CIFAR-10 Batch 5:  loss: 0.0080, acc: 1.0000, valid_loss: 0.9438, valid_acc: 0.8464, \n",
      "Epoch 89, CIFAR-10 Batch 1:  loss: 0.0030, acc: 1.0000, valid_loss: 0.9735, valid_acc: 0.8490, \n",
      "Epoch 89, CIFAR-10 Batch 2:  loss: 0.0015, acc: 1.0000, valid_loss: 1.0601, valid_acc: 0.8556, \n",
      "Epoch 89, CIFAR-10 Batch 3:  loss: 0.0022, acc: 1.0000, valid_loss: 1.1227, valid_acc: 0.8528, \n",
      "Epoch 89, CIFAR-10 Batch 4:  loss: 0.0021, acc: 0.9988, valid_loss: 1.1500, valid_acc: 0.8488, \n",
      "Epoch 89, CIFAR-10 Batch 5:  loss: 0.0084, acc: 0.9988, valid_loss: 0.9854, valid_acc: 0.8532, \n",
      "Epoch 90, CIFAR-10 Batch 1:  loss: 0.0068, acc: 0.9975, valid_loss: 0.9352, valid_acc: 0.8546, \n",
      "Epoch 90, CIFAR-10 Batch 2:  loss: 0.0034, acc: 1.0000, valid_loss: 0.9123, valid_acc: 0.8568, \n",
      "Epoch 90, CIFAR-10 Batch 3:  loss: 0.0015, acc: 1.0000, valid_loss: 1.0018, valid_acc: 0.8584, \n",
      "Epoch 90, CIFAR-10 Batch 4:  loss: 0.0005, acc: 1.0000, valid_loss: 1.2207, valid_acc: 0.8480, \n",
      "Epoch 90, CIFAR-10 Batch 5:  loss: 0.0011, acc: 1.0000, valid_loss: 1.2804, valid_acc: 0.8504, \n",
      "Epoch 91, CIFAR-10 Batch 1:  loss: 0.0020, acc: 1.0000, valid_loss: 1.0946, valid_acc: 0.8526, \n",
      "Epoch 91, CIFAR-10 Batch 2:  loss: 0.0013, acc: 1.0000, valid_loss: 1.0940, valid_acc: 0.8486, \n",
      "Epoch 91, CIFAR-10 Batch 3:  loss: 0.0020, acc: 0.9988, valid_loss: 0.9944, valid_acc: 0.8536, \n",
      "Epoch 91, CIFAR-10 Batch 4:  loss: 0.0035, acc: 1.0000, valid_loss: 1.0030, valid_acc: 0.8420, \n",
      "Epoch 91, CIFAR-10 Batch 5:  loss: 0.0029, acc: 1.0000, valid_loss: 1.0312, valid_acc: 0.8566, \n",
      "Epoch 92, CIFAR-10 Batch 1:  loss: 0.0010, acc: 1.0000, valid_loss: 1.1646, valid_acc: 0.8568, \n",
      "Epoch 92, CIFAR-10 Batch 2:  loss: 0.0022, acc: 1.0000, valid_loss: 1.2342, valid_acc: 0.8492, \n",
      "Epoch 92, CIFAR-10 Batch 3:  loss: 0.0022, acc: 1.0000, valid_loss: 1.0479, valid_acc: 0.8426, \n",
      "Epoch 92, CIFAR-10 Batch 4:  loss: 0.0029, acc: 1.0000, valid_loss: 1.0731, valid_acc: 0.8400, \n",
      "Epoch 92, CIFAR-10 Batch 5:  loss: 0.0101, acc: 0.9988, valid_loss: 1.3950, valid_acc: 0.8408, \n",
      "Epoch 93, CIFAR-10 Batch 1:  loss: 0.0041, acc: 1.0000, valid_loss: 1.1608, valid_acc: 0.8416, \n",
      "Epoch 93, CIFAR-10 Batch 2:  loss: 0.0086, acc: 0.9988, valid_loss: 1.0252, valid_acc: 0.8402, \n",
      "Epoch 93, CIFAR-10 Batch 3:  loss: 0.0083, acc: 0.9975, valid_loss: 0.9197, valid_acc: 0.8454, \n",
      "Epoch 93, CIFAR-10 Batch 4:  loss: 0.0181, acc: 0.9950, valid_loss: 1.0221, valid_acc: 0.8460, \n",
      "Epoch 93, CIFAR-10 Batch 5:  loss: 0.0029, acc: 1.0000, valid_loss: 0.9709, valid_acc: 0.8476, \n",
      "Epoch 94, CIFAR-10 Batch 1:  loss: 0.0057, acc: 0.9988, valid_loss: 0.9848, valid_acc: 0.8494, \n",
      "Epoch 94, CIFAR-10 Batch 2:  loss: 0.0045, acc: 0.9988, valid_loss: 1.0343, valid_acc: 0.8458, \n",
      "Epoch 94, CIFAR-10 Batch 3:  loss: 0.0043, acc: 0.9988, valid_loss: 0.9523, valid_acc: 0.8496, \n",
      "Epoch 94, CIFAR-10 Batch 4:  loss: 0.0023, acc: 1.0000, valid_loss: 1.0217, valid_acc: 0.8500, \n",
      "Epoch 94, CIFAR-10 Batch 5:  loss: 0.0015, acc: 1.0000, valid_loss: 0.9646, valid_acc: 0.8486, \n",
      "Epoch 95, CIFAR-10 Batch 1:  loss: 0.0019, acc: 1.0000, valid_loss: 1.1241, valid_acc: 0.8478, \n",
      "Epoch 95, CIFAR-10 Batch 2:  loss: 0.0023, acc: 1.0000, valid_loss: 1.1533, valid_acc: 0.8506, \n",
      "Epoch 95, CIFAR-10 Batch 3:  loss: 0.0012, acc: 1.0000, valid_loss: 1.0781, valid_acc: 0.8536, \n",
      "Epoch 95, CIFAR-10 Batch 4:  loss: 0.0057, acc: 0.9975, valid_loss: 1.0190, valid_acc: 0.8504, \n",
      "Epoch 95, CIFAR-10 Batch 5:  loss: 0.0010, acc: 1.0000, valid_loss: 1.0678, valid_acc: 0.8448, \n",
      "Epoch 96, CIFAR-10 Batch 1:  loss: 0.0014, acc: 1.0000, valid_loss: 1.1403, valid_acc: 0.8514, \n",
      "Epoch 96, CIFAR-10 Batch 2:  loss: 0.0048, acc: 0.9988, valid_loss: 1.2200, valid_acc: 0.8478, \n",
      "Epoch 96, CIFAR-10 Batch 3:  loss: 0.0029, acc: 1.0000, valid_loss: 1.1929, valid_acc: 0.8504, \n",
      "Epoch 96, CIFAR-10 Batch 4:  loss: 0.0030, acc: 1.0000, valid_loss: 1.1063, valid_acc: 0.8532, \n",
      "Epoch 96, CIFAR-10 Batch 5:  loss: 0.0027, acc: 0.9988, valid_loss: 1.0488, valid_acc: 0.8518, \n",
      "Epoch 97, CIFAR-10 Batch 1:  loss: 0.0066, acc: 0.9988, valid_loss: 1.0429, valid_acc: 0.8496, \n",
      "Epoch 97, CIFAR-10 Batch 2:  loss: 0.0026, acc: 1.0000, valid_loss: 1.0520, valid_acc: 0.8526, \n",
      "Epoch 97, CIFAR-10 Batch 3:  loss: 0.0025, acc: 1.0000, valid_loss: 1.0873, valid_acc: 0.8546, \n",
      "Epoch 97, CIFAR-10 Batch 4:  loss: 0.0044, acc: 1.0000, valid_loss: 1.0328, valid_acc: 0.8462, \n",
      "Epoch 97, CIFAR-10 Batch 5:  loss: 0.0011, acc: 1.0000, valid_loss: 1.0073, valid_acc: 0.8522, \n",
      "Epoch 98, CIFAR-10 Batch 1:  loss: 0.0021, acc: 1.0000, valid_loss: 1.0562, valid_acc: 0.8492, \n",
      "Epoch 98, CIFAR-10 Batch 2:  loss: 0.0015, acc: 1.0000, valid_loss: 1.0419, valid_acc: 0.8494, \n",
      "Epoch 98, CIFAR-10 Batch 3:  loss: 0.0013, acc: 1.0000, valid_loss: 1.0119, valid_acc: 0.8580, \n",
      "Epoch 98, CIFAR-10 Batch 4:  loss: 0.0012, acc: 1.0000, valid_loss: 1.2424, valid_acc: 0.8532, \n",
      "Epoch 98, CIFAR-10 Batch 5:  loss: 0.0003, acc: 1.0000, valid_loss: 1.2228, valid_acc: 0.8552, \n",
      "Epoch 99, CIFAR-10 Batch 1:  loss: 0.0007, acc: 1.0000, valid_loss: 1.2295, valid_acc: 0.8562, \n",
      "Epoch 99, CIFAR-10 Batch 2:  loss: 0.0015, acc: 1.0000, valid_loss: 1.2313, valid_acc: 0.8558, \n",
      "Epoch 99, CIFAR-10 Batch 3:  loss: 0.0004, acc: 1.0000, valid_loss: 1.2669, valid_acc: 0.8572, \n",
      "Epoch 99, CIFAR-10 Batch 4:  loss: 0.0004, acc: 1.0000, valid_loss: 1.1867, valid_acc: 0.8548, \n",
      "Epoch 99, CIFAR-10 Batch 5:  loss: 0.0009, acc: 1.0000, valid_loss: 1.1914, valid_acc: 0.8540, \n",
      "Epoch 100, CIFAR-10 Batch 1:  loss: 0.0012, acc: 1.0000, valid_loss: 1.1221, valid_acc: 0.8498, \n",
      "Epoch 100, CIFAR-10 Batch 2:  loss: 0.0013, acc: 1.0000, valid_loss: 1.1266, valid_acc: 0.8530, \n",
      "Epoch 100, CIFAR-10 Batch 3:  loss: 0.0020, acc: 1.0000, valid_loss: 1.2063, valid_acc: 0.8486, \n",
      "Epoch 100, CIFAR-10 Batch 4:  loss: 0.0014, acc: 1.0000, valid_loss: 1.2785, valid_acc: 0.8444, \n",
      "Epoch 100, CIFAR-10 Batch 5:  loss: 0.0024, acc: 1.0000, valid_loss: 1.0345, valid_acc: 0.8498, \n"
     ]
    }
   ],
   "source": [
    "# 减少epochs为100节省训练时间\n",
    "epochs = 100\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            print(\"\")\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点\n",
    "\n",
    "模型已保存到本地。\n",
    "\n",
    "## 测试模型\n",
    "\n",
    "利用测试数据集测试你的模型。这将是最终的准确率。你的准确率应该高于 50%。如果没达到，请继续调整模型结构和参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.8510343551635742\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fef98383438>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为何准确率只有50-80%？\n",
    "\n",
    "你可能想问，为何准确率不能更高了？首先，对于简单的 CNN 网络来说，50% 已经不低了。纯粹猜测的准确率为10%。但是，你可能注意到有人的准确率[远远超过 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130)。这是因为我们还没有介绍所有的神经网络知识。我们还需要掌握一些其他技巧。\n",
    "\n",
    "## 提交项目\n",
    "\n",
    "提交项目时，确保先运行所有单元，然后再保存记事本。将 notebook 文件另存为“dlnd_image_classification.ipynb”，再在目录 \"File\" -> \"Download as\" 另存为 HTML 格式。请在提交的项目中包含 “helper.py” 和 “problem_unittests.py” 文件。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
